{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODTBr1wPagBYFmJK8BDKbF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinjin0309/DATATHON_6/blob/main/KoELECTRA_%EB%A6%AC%EB%B7%B0_%EA%B0%90%EC%84%B1_%EB%B6%84%EC%84%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# 1ï¸âƒ£ CSV ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "df = pd.read_csv('/content/everland_reviews (1).csv')\n",
        "\n",
        "# 2ï¸âƒ£ ë¦¬ë·° í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…\n",
        "text_col = 'content'   # ì‹¤ì œ íŒŒì¼ ì»¬ëŸ¼ëª… ë°˜ì˜ âœ…\n",
        "\n",
        "# 3ï¸âƒ£ í…Œë§ˆë³„ í‚¤ì›Œë“œ ì‚¬ì „ ì •ì˜\n",
        "theme_keywords = {\n",
        "    'Service': ['ì§ì›','ì¹œì ˆ','ë¶ˆì¹œì ˆ','ì‘ëŒ€','ì„œë¹„ìŠ¤','íƒœë„'],\n",
        "    'Facility': ['í™”ì¥ì‹¤','ì²­ê²°','ê´€ë¦¬','ê¹¨ë—','ì •ë¹„','ì‹œì„¤'],\n",
        "    'Price': ['ê°€ê²©','ë¹„ì‹¸','ì¿ í°','í• ì¸','ì…ì¥ë£Œ'],\n",
        "    'Waiting': ['ëŒ€ê¸°','ì¤„','ì›¨ì´íŒ…','ë¶ë¹”','ê¸°ë‹¤ë¦¼'],\n",
        "    'Attractions': ['ë†€ì´ê¸°êµ¬','í¼ë ˆì´ë“œ','ê³µì—°','ìŠ¤ë¦´','ì¬ë°Œ','ì²´í—˜']\n",
        "}\n",
        "\n",
        "# 4ï¸âƒ£ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ê°•í™” ë²„ì „)\n",
        "stopwords = set([\n",
        "    # ì¡°ì‚¬ / ì ‘ì†ì‚¬ / ê´€ì‚¬\n",
        "    'ì€','ëŠ”','ì´','ê°€','ì„','ë¥¼','ë„','ì—','ì™€','ê³¼','ì˜','ë¡œ','ìœ¼ë¡œ','ì—ì„œ','ì—ê²Œ','ë³´ë‹¤','ìœ¼ë¡œì„œ',\n",
        "    'ë˜','ë˜í•œ','ê·¸ë¦¬ê³ ','í•˜ì§€ë§Œ','ê·¸ë˜ì„œ','ê·¸ëŸ¬ë‚˜','ë•Œë¬¸','ë•Œë¬¸ì—','ì´ëŸ°','ì €ëŸ°','ê·¸ëŸ°','ìš”','ë‹¤','ë“¯','ë“¯ì´',\n",
        "    # ë¶€ì‚¬ / í˜•ìš©ì‚¬ì„± ë¶ˆìš©ì–´\n",
        "    'ë„ˆë¬´','ë§¤ìš°','ì •ë§','ì§„ì§œ','ì•„ì£¼','êµ‰ì¥íˆ','ì—„ì²­','ë§ì´','ë§ì•„ì„œ','ë§ê³ ','ë§ì€','ì¢‹ì€','ì¢‹ì•„ìš”','ì¢‹ì•˜ì–´ìš”',\n",
        "    'ìµœê³ ','ìµœê³ ì˜','ì—­ì‹œ','ìˆëŠ”','ìˆì–´ì„œ','ìˆê³ ','ê°™ì€','ê°™ì•„ìš”','ê°™ì´','ê·¸ëƒ¥','ì¢€','ì¡°ê¸ˆ','ì•½ê°„',\n",
        "    'ì¬ë°ŒëŠ”','ì¦ê±°ìš´','ì¦ê±°ì› ì–´ìš”','í–‰ë³µí•œ','í–‰ë³µí–ˆì–´ìš”',\n",
        "    # ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ / ìì£¼ ë°˜ë³µë˜ëŠ” í‘œí˜„\n",
        "    'ì‚¬ëŒ','ì‚¬ëŒì´','ì‚¬ëŒë“¤','ì—ë²„ëœë“œ','ì—ë²„ëœë“œëŠ”','ê³³','ê³³ì´','ê³³ë„','ì •ë§ë¡œ','ë˜ëŠ”','í˜¹ì€','ë‹¤ë…€ì™”ì–´ìš”',\n",
        "    'íƒ€ê³ ','íƒ”ì–´ìš”','ê°€ì„œ','ì™”ë‹¤','ì™”ë‹¤ê°€','í–ˆìŠµë‹ˆë‹¤','í–ˆì–´ìš”','ë³´ë‹ˆ','ë³´ë‹ˆê¹Œ','ì´ìš©','ì´ìš©í–ˆì–´ìš”',\n",
        "    'ê°€ê¸°','ê°”ë‹¤','ê°€ëŠ”','ê°€ì„œ','ì™”ì–´ìš”','ê°”ì–´ìš”','í–ˆì–´ìš”'\n",
        "])\n",
        "\n",
        "# 5ï¸âƒ£ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
        "all_text = \" \".join(df[text_col].astype(str)).replace('\\n', ' ')\n",
        "all_text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', ' ', all_text)\n",
        "\n",
        "# 6ï¸âƒ£ í† í°í™” + ë¶ˆìš©ì–´ ì œê±°\n",
        "words = [w for w in all_text.split() if w not in stopwords and len(w) > 1]\n",
        "\n",
        "# 7ï¸âƒ£ ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°\n",
        "word_freq = Counter(words)\n",
        "\n",
        "# 8ï¸âƒ£ í…Œë§ˆë³„ í‚¤ì›Œë“œ ë“±ì¥ íšŸìˆ˜ ê³„ì‚°\n",
        "theme_counts = {}\n",
        "for theme, keywords in theme_keywords.items():\n",
        "    count = sum(word_freq[k] for k in keywords if k in word_freq)\n",
        "    theme_counts[theme] = count\n",
        "\n",
        "# 9ï¸âƒ£ í…Œë§ˆë³„ ê²°ê³¼ ìš”ì•½\n",
        "theme_df = pd.DataFrame(list(theme_counts.items()), columns=['Theme', 'KeywordCount']).sort_values('KeywordCount', ascending=False)\n",
        "print(\"=== ğŸ” í…Œë§ˆë³„ í‚¤ì›Œë“œ ë“±ì¥ íšŸìˆ˜ ===\")\n",
        "print(theme_df)\n",
        "\n",
        "# ğŸ”Ÿ ì „ì²´ ë‹¨ì–´ ìƒìœ„ 20ê°œ\n",
        "top_words = pd.DataFrame(word_freq.most_common(20), columns=['Word', 'Frequency'])\n",
        "print(\"\\n=== ğŸ·ï¸ ì „ì²´ ë¦¬ë·° ë‚´ ë‹¨ì–´ ë¹ˆë„ Top 20 ===\")\n",
        "print(top_words)\n"
      ],
      "metadata": {
        "id": "gMj9uz23fn9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# 1ï¸âƒ£ CSV ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "df = pd.read_csv('/content/everland_reviews (1).csv')\n",
        "\n",
        "# 2ï¸âƒ£ ë¦¬ë·° í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª… (ì‹¤ì œ ë°ì´í„° ë°˜ì˜)\n",
        "text_col = 'content'\n",
        "\n",
        "# 3ï¸âƒ£ í…Œë§ˆë³„ í‚¤ì›Œë“œ ì‚¬ì „ (ë¶€ì • ë¶„ì„ìš©ì—ë„ ì‚¬ìš©)\n",
        "theme_keywords = {\n",
        "    'Service': ['ë¶ˆì¹œì ˆ','ë¶ˆë§Œ','ì‘ëŒ€','íƒœë„','ì§ì›','ì„œë¹„ìŠ¤'],\n",
        "    'Facility': ['ë”ëŸ½','ëƒ„ìƒˆ','ë¶ˆí¸','ê³ ì¥','ê´€ë¦¬','ì²­ê²°','ì‹œì„¤'],\n",
        "    'Price': ['ë¹„ì‹¸','ë¹„ìŒˆ','ê°€ê²©','ì¿ í°','í• ì¸ì—†ìŒ','ì…ì¥ë£Œ'],\n",
        "    'Waiting': ['ëŒ€ê¸°','ì¤„','ì›¨ì´íŒ…','ë¶ë¹”','í˜¼ì¡','ê¸°ë‹¤ë¦¼','ëŠ¦ìŒ'],\n",
        "    'Attractions': ['ê³ ì¥','ë©ˆì¶¤','ìŠ¤ë¦´ì—†','ì§€ë£¨','ë³„ë¡œ','ì‹¤ë§','ì¬ë¯¸ì—†']\n",
        "}\n",
        "\n",
        "# 4ï¸âƒ£ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ì•ì„  ì½”ë“œì˜ ê°•í™” ë²„ì „ ì¬ì‚¬ìš©)\n",
        "stopwords = set([\n",
        "    'ì€','ëŠ”','ì´','ê°€','ì„','ë¥¼','ë„','ì—','ì™€','ê³¼','ì˜','ë¡œ','ìœ¼ë¡œ','ì—ì„œ','ì—ê²Œ','ë³´ë‹¤','ìœ¼ë¡œì„œ',\n",
        "    'ë˜','ë˜í•œ','ê·¸ë¦¬ê³ ','í•˜ì§€ë§Œ','ê·¸ë˜ì„œ','ê·¸ëŸ¬ë‚˜','ë•Œë¬¸','ë•Œë¬¸ì—','ì´ëŸ°','ì €ëŸ°','ê·¸ëŸ°','ìš”','ë‹¤','ë“¯','ë“¯ì´',\n",
        "    'ë„ˆë¬´','ë§¤ìš°','ì •ë§','ì§„ì§œ','ì•„ì£¼','êµ‰ì¥íˆ','ì—„ì²­','ë§ì´','ë§ì•„ì„œ','ë§ê³ ','ë§ì€','ì¢‹ì€','ì¢‹ì•„ìš”','ì¢‹ì•˜ì–´ìš”',\n",
        "    'ìµœê³ ','ìµœê³ ì˜','ì—­ì‹œ','ìˆëŠ”','ìˆì–´ì„œ','ìˆê³ ','ê°™ì€','ê°™ì•„ìš”','ê°™ì´','ê·¸ëƒ¥','ì¢€','ì¡°ê¸ˆ','ì•½ê°„',\n",
        "    'ì¬ë°ŒëŠ”','ì¦ê±°ìš´','ì¦ê±°ì› ì–´ìš”','í–‰ë³µí•œ','í–‰ë³µí–ˆì–´ìš”',\n",
        "    'ì‚¬ëŒ','ì‚¬ëŒì´','ì‚¬ëŒë“¤','ì—ë²„ëœë“œ','ì—ë²„ëœë“œëŠ”','ê³³','ê³³ì´','ê³³ë„','ì •ë§ë¡œ','ë˜ëŠ”','í˜¹ì€','ë‹¤ë…€ì™”ì–´ìš”',\n",
        "    'íƒ€ê³ ','íƒ”ì–´ìš”','ê°€ì„œ','ì™”ë‹¤','ì™”ë‹¤ê°€','í–ˆìŠµë‹ˆë‹¤','í–ˆì–´ìš”','ë³´ë‹ˆ','ë³´ë‹ˆê¹Œ','ì´ìš©','ì´ìš©í–ˆì–´ìš”',\n",
        "    'ê°€ê¸°','ê°”ë‹¤','ê°€ëŠ”','ê°€ì„œ','ì™”ì–´ìš”','ê°”ì–´ìš”','í–ˆì–´ìš”'\n",
        "])\n",
        "\n",
        "# 5ï¸âƒ£ ë¶€ì • ë‹¨ì–´ ì‚¬ì „ ì •ì˜\n",
        "negative_words = set([\n",
        "    'ë¶ˆì¹œì ˆ','ë¶ˆë§Œ','ë¹„ì‹¸','ë¹„ìŒˆ','ëŒ€ê¸°','ì¤„','ì›¨ì´íŒ…','ë¶ë¹”','í˜¼ì¡','ê¸°ë‹¤ë¦¼',\n",
        "    'ì§€ë£¨','ê³ ì¥','ëŠ¦ìŒ','ë³„ë¡œ','ì‹¤ë§','ë”ëŸ½','ëƒ„ìƒˆ','ë¶ˆí¸','ìœ„í—˜','ì§œì¦','ìµœì•…','ë³„ë¡œ','ì•„ì‰¬ì›€',\n",
        "    'ë³µì¡','ì‹œë„ëŸ½','ë‹µë‹µ','ì¢ìŒ','ì–´ë‘¡','ë¶ˆì¾Œ','ë¹„íš¨ìœ¨','í˜ë“¦','ë¥ë‹¤','ì¶”ì›€','ì–´ìˆ˜ì„ '\n",
        "])\n",
        "\n",
        "# 6ï¸âƒ£ í…ìŠ¤íŠ¸ ì •ì œ\n",
        "all_text = \" \".join(df[text_col].astype(str)).replace('\\n', ' ')\n",
        "all_text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', ' ', all_text)\n",
        "\n",
        "# 7ï¸âƒ£ í† í°í™” + ë¶ˆìš©ì–´ ì œê±°\n",
        "words = [w for w in all_text.split() if w not in stopwords and len(w) > 1]\n",
        "\n",
        "# 8ï¸âƒ£ ë¶€ì • ë‹¨ì–´ë§Œ í•„í„°ë§\n",
        "neg_words = [w for w in words if w in negative_words]\n",
        "neg_freq = Counter(neg_words)\n",
        "\n",
        "# 9ï¸âƒ£ í…Œë§ˆë³„ ë¶€ì • ë‹¨ì–´ ë“±ì¥ íšŸìˆ˜ ê³„ì‚°\n",
        "theme_neg_counts = {}\n",
        "for theme, keywords in theme_keywords.items():\n",
        "    count = sum(neg_freq[k] for k in keywords if k in neg_freq)\n",
        "    theme_neg_counts[theme] = count\n",
        "\n",
        "# ğŸ”Ÿ ê²°ê³¼ ìš”ì•½\n",
        "theme_neg_df = pd.DataFrame(list(theme_neg_counts.items()), columns=['Theme', 'NegativeKeywordCount']).sort_values('NegativeKeywordCount', ascending=False)\n",
        "print(\"=== ğŸ” í…Œë§ˆë³„ ë¶€ì • í‚¤ì›Œë“œ ë“±ì¥ íšŸìˆ˜ ===\")\n",
        "print(theme_neg_df)\n",
        "\n",
        "# â“« ì „ì²´ ë¶€ì • ë‹¨ì–´ ìƒìœ„ 20ê°œ\n",
        "top_neg_words = pd.DataFrame(neg_freq.most_common(20), columns=['Word', 'Frequency'])\n",
        "print(\"\\n=== ğŸ’¢ ì „ì²´ ë¦¬ë·° ë‚´ ë¶€ì • ë‹¨ì–´ Top 20 ===\")\n",
        "print(top_neg_words)\n"
      ],
      "metadata": {
        "id": "XDRObyt_f6EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "df = pd.read_csv('/content/everland_reviews (1).csv')\n",
        "text_col = 'content'\n",
        "\n",
        "# ğŸ§© í™•ì¥ í…Œë§ˆ í‚¤ì›Œë“œ\n",
        "theme_keywords = {\n",
        "    'Service': ['ì§ì›','ì¹œì ˆ','ë¶ˆì¹œì ˆ','ì‘ëŒ€','ì„œë¹„ìŠ¤','íƒœë„','ì†Œí†µ','ê³ ê°ì„¼í„°','ì•ˆë‚´','ë§¤ë‹ˆì €','ì§ì›íƒœë„'],\n",
        "    'Facility': ['í™”ì¥ì‹¤','ì²­ê²°','ê´€ë¦¬','ê¹¨ë—','ì •ë¹„','ì‹œì„¤','ë…¸í›„','ìœ„í—˜','ìœ„ìƒ','ê³ ì¥','ì •ë¦¬','ë¶„ë¦¬ìˆ˜ê±°'],\n",
        "    'Price': ['ê°€ê²©','ë¹„ì‹¸','ë¹„ìŒˆ','ì¿ í°','í• ì¸','ì…ì¥ë£Œ','ê°€ì„±ë¹„','ë¹„ìš©','ëˆ','ê°€ê²©ëŒ€','ê°€ê²©í‘œ'],\n",
        "    'Waiting': ['ëŒ€ê¸°','ì¤„','ì›¨ì´íŒ…','ë¶ë¹”','í˜¼ì¡','ê¸°ë‹¤ë¦¼','ì§€ì—°','ì •ì²´','ì¤„ì„œë‹¤','ëŒ€ê¸°ì‹œê°„','ì‹œê°„'],\n",
        "    'Attractions': ['ë†€ì´ê¸°êµ¬','í¼ë ˆì´ë“œ','ê³µì—°','ìŠ¤ë¦´','ì¬ë°Œ','ì²´í—˜','ê³ ì¥','ìš´íœ´','ì§§ìŒ','ì§€ë£¨','ë³„ë¡œ','ì‹¤ë§']\n",
        "}\n",
        "\n",
        "# ğŸ§¹ ë¶ˆìš©ì–´ (ì´ì „ ê°•í™” ë²„ì „ ê·¸ëŒ€ë¡œ ì‚¬ìš©)\n",
        "stopwords = set([\n",
        "    'ì€','ëŠ”','ì´','ê°€','ì„','ë¥¼','ë„','ì—','ì™€','ê³¼','ì˜','ë¡œ','ìœ¼ë¡œ','ì—ì„œ','ì—ê²Œ','ë³´ë‹¤','ìœ¼ë¡œì„œ',\n",
        "    'ë˜','ë˜í•œ','ê·¸ë¦¬ê³ ','í•˜ì§€ë§Œ','ê·¸ë˜ì„œ','ê·¸ëŸ¬ë‚˜','ë•Œë¬¸','ë•Œë¬¸ì—','ì´ëŸ°','ì €ëŸ°','ê·¸ëŸ°','ìš”','ë‹¤','ë“¯','ë“¯ì´',\n",
        "    'ë„ˆë¬´','ë§¤ìš°','ì •ë§','ì§„ì§œ','ì•„ì£¼','êµ‰ì¥íˆ','ì—„ì²­','ë§ì´','ë§ì•„ì„œ','ë§ê³ ','ë§ì€','ì¢‹ì€','ì¢‹ì•„ìš”','ì¢‹ì•˜ì–´ìš”',\n",
        "    'ìµœê³ ','ìµœê³ ì˜','ì—­ì‹œ','ìˆëŠ”','ìˆì–´ì„œ','ìˆê³ ','ê°™ì€','ê°™ì•„ìš”','ê°™ì´','ê·¸ëƒ¥','ì¢€','ì¡°ê¸ˆ','ì•½ê°„','ì •ë„',\n",
        "    'í•­ìƒ','ê°€ë”','ì–¸ì œë‚˜','ì •ë§ë¡œ','ê·¸ë ‡ë‹¤','ì´ë ‡ë‹¤','ã…‹ã…‹','ã…ã…','ã… ã… ','ã…œã…œ','ì™€','ìš°ì™€','í—','í•˜í•˜','ì•¼',\n",
        "    'ì•„','ìŒ','ì‘','ì—íœ´','íœ´','í•˜','ì–´íœ´','ì§±','ì‚¬ëŒ','ì‚¬ëŒì´','ì‚¬ëŒë“¤','ì—ë²„ëœë“œ','ì—ë²„ëœë“œëŠ”','ê³³','ê³³ì´',\n",
        "    'ê³³ë„','ì •ë§ë¡œ','ë˜ëŠ”','í˜¹ì€','ë‹¤ë…€ì™”ì–´ìš”','íƒ€ê³ ','íƒ”ì–´ìš”','ê°€ì„œ','ì™”ë‹¤','ì™”ë‹¤ê°€','í–ˆìŠµë‹ˆë‹¤','í–ˆì–´ìš”',\n",
        "    'ë³´ë‹ˆ','ë³´ë‹ˆê¹Œ','ì´ìš©','ì´ìš©í–ˆì–´ìš”','ê°€ê¸°','ê°”ë‹¤','ê°€ëŠ”','ê°€ì„œ','ì™”ì–´ìš”','ê°”ì–´ìš”','í–ˆì–´ìš”','ë´¤ì–´ìš”',\n",
        "    'ë´ì„œ','ë´¤ë‹¤','ë´¤ìŠµë‹ˆë‹¤'\n",
        "])\n",
        "\n",
        "# ğŸ”§ í…ìŠ¤íŠ¸ ì •ì œ\n",
        "all_text = \" \".join(df[text_col].astype(str)).replace('\\n',' ')\n",
        "all_text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', ' ', all_text)\n",
        "\n",
        "# ğŸ”¤ í† í°í™” + ë¶ˆìš©ì–´ ì œê±°\n",
        "words = [w for w in all_text.split() if w not in stopwords and len(w) > 1]\n",
        "word_freq = Counter(words)\n",
        "\n",
        "# ğŸ§® í…Œë§ˆë³„ ë¹ˆë„ í•©ì‚°\n",
        "theme_counts = {t: sum(word_freq[k] for k in ks if k in word_freq)\n",
        "                for t, ks in theme_keywords.items()}\n",
        "\n",
        "# ğŸ“Š ì¶œë ¥\n",
        "theme_df = pd.DataFrame(list(theme_counts.items()), columns=['Theme','KeywordCount']).sort_values('KeywordCount',ascending=False)\n",
        "top_words = pd.DataFrame(word_freq.most_common(50), columns=['Word','Frequency'])\n",
        "\n",
        "print(\"=== ğŸ” í…Œë§ˆë³„ ì „ì²´ í‚¤ì›Œë“œ ë“±ì¥ íšŸìˆ˜ ===\")\n",
        "print(theme_df)\n",
        "print(\"\\n=== ğŸ·ï¸ ì „ì²´ ë¦¬ë·° ë‚´ ë‹¨ì–´ ë¹ˆë„ Top 50 ===\")\n",
        "print(top_words)\n"
      ],
      "metadata": {
        "id": "1KvkZO6WhWCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "df = pd.read_csv('/content/everland_reviews (1).csv')\n",
        "text_col = 'content'\n",
        "\n",
        "# ğŸ§© í™•ì¥ í…Œë§ˆ í‚¤ì›Œë“œ (ë¶€ì • ì¤‘ì‹¬)\n",
        "theme_keywords = {\n",
        "    'Service': ['ë¶ˆì¹œì ˆ','ë¶ˆë§Œ','ì‘ëŒ€','íƒœë„','ì§ì›','ì„œë¹„ìŠ¤','ì†Œí†µ','ì•ˆë‚´','ê³ ê°ì„¼í„°'],\n",
        "    'Facility': ['ë”ëŸ½','ëƒ„ìƒˆ','ë¶ˆí¸','ê³ ì¥','ê´€ë¦¬','ì²­ê²°','ì‹œì„¤','ì •ë¹„','ë…¸í›„','ìœ„í—˜','ìœ„ìƒ'],\n",
        "    'Price': ['ë¹„ì‹¸','ë¹„ìŒˆ','ê°€ê²©','ì¿ í°','í• ì¸ì—†ìŒ','ì…ì¥ë£Œ','ê°€ì„±ë¹„','ë¹„ìš©','ë¹„íš¨ìœ¨','ëˆ'],\n",
        "    'Waiting': ['ëŒ€ê¸°','ì¤„','ì›¨ì´íŒ…','ë¶ë¹”','í˜¼ì¡','ê¸°ë‹¤ë¦¼','ëŠ¦ìŒ','ì§€ì—°','ì •ì²´','ê¸¸ë‹¤','ì‹œê°„'],\n",
        "    'Attractions': ['ê³ ì¥','ë©ˆì¶¤','ìŠ¤ë¦´ì—†','ì§€ë£¨','ë³„ë¡œ','ì‹¤ë§','ì¬ë¯¸ì—†','ìš´íœ´','ì§§ìŒ','ë¶ˆì¾Œ','ì§€ì €ë¶„']\n",
        "}\n",
        "\n",
        "# ğŸ§¹ ë¶ˆìš©ì–´ (ë™ì¼)\n",
        "stopwords = set([...])  # ìœ„ì™€ ë™ì¼ ë¦¬ìŠ¤íŠ¸ ë³µë¶™\n",
        "\n",
        "# ğŸ’¢ ë¶€ì • ë‹¨ì–´ í™•ì¥\n",
        "negative_words = set([\n",
        "    'ë¶ˆì¹œì ˆ','ë¶ˆë§Œ','ë¹„ì‹¸','ë¹„ìŒˆ','ëŒ€ê¸°','ì¤„','ì›¨ì´íŒ…','ë¶ë¹”','í˜¼ì¡','ê¸°ë‹¤ë¦¼','ì§€ë£¨','ê³ ì¥','ëŠ¦ìŒ','ë³„ë¡œ','ì‹¤ë§',\n",
        "    'ë”ëŸ½','ëƒ„ìƒˆ','ë¶ˆí¸','ìœ„í—˜','ì§œì¦','ìµœì•…','ë³µì¡','ì‹œë„ëŸ½','ë‹µë‹µ','ì¢ìŒ','ì–´ë‘¡','ë¶ˆì¾Œ','ë¹„íš¨ìœ¨','í˜ë“¦','ë¥ë‹¤',\n",
        "    'ì¶”ì›€','ì–´ìˆ˜ì„ ','ì—‰ë§','ì§€ì €ë¶„','ëƒ„ìƒˆë‚¨','ì—‰ì„±','ë‚¡ìŒ','ëŠë¦¼','ë§‰í˜','ëŠê¹€','í˜¼ì¡í•¨','ë¶ˆì¾Œê°','ìœ„ìƒë¬¸ì œ',\n",
        "    'ê´€ë¦¬ë¶€ì¡±','ì •ì²´','ê¸¸ë‹¤','ë¶ˆì¾Œí•˜ë‹¤','ë¶ˆì¾Œí–ˆ','ë¶ˆë§Œì¡±','ë¶ˆí¸í•¨','í˜ë“¤ë‹¤','í˜ë“¤ì—ˆ','ì§œì¦ë‚¨','ìš´íœ´','ë…¸í›„',\n",
        "    'ì¤„ì„œë‹¤','ì§€ì—°','ë¹„íš¨ìœ¨ì ','ë„ˆë¬´ë¹„ì‹¸','ë³„ë¡œì˜€','ë³„ë¡œì—ìš”','ë¹„ìŒŒì–´ìš”'\n",
        "])\n",
        "\n",
        "# ğŸ”§ í…ìŠ¤íŠ¸ ì •ì œ\n",
        "all_text = \" \".join(df[text_col].astype(str)).replace('\\n',' ')\n",
        "all_text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', ' ', all_text)\n",
        "\n",
        "# ğŸ”¤ í† í°í™” + ë¶ˆìš©ì–´ ì œê±°\n",
        "words = [w for w in all_text.split() if w not in stopwords and len(w) > 1]\n",
        "\n",
        "# ğŸ’¢ ë¶€ì • ë‹¨ì–´ë§Œ í•„í„°ë§\n",
        "neg_words = [w for w in words if w in negative_words]\n",
        "neg_freq = Counter(neg_words)\n",
        "\n",
        "# ğŸ§® í…Œë§ˆë³„ ë¶€ì • ë‹¨ì–´ í•©ì‚°\n",
        "theme_neg_counts = {t: sum(neg_freq[k] for k in ks if k in neg_freq)\n",
        "                    for t, ks in theme_keywords.items()}\n",
        "\n",
        "# ğŸ“Š ì¶œë ¥\n",
        "theme_neg_df = pd.DataFrame(list(theme_neg_counts.items()), columns=['Theme','NegativeKeywordCount']).sort_values('NegativeKeywordCount',ascending=False)\n",
        "top_neg_words = pd.DataFrame(neg_freq.most_common(50), columns=['Word','Frequency'])\n",
        "\n",
        "print(\"=== ğŸ’¢ í…Œë§ˆë³„ ë¶€ì • í‚¤ì›Œë“œ ë“±ì¥ íšŸìˆ˜ ===\")\n",
        "print(theme_neg_df)\n",
        "print(\"\\n=== ğŸ’¢ ì „ì²´ ë¦¬ë·° ë‚´ ë¶€ì • ë‹¨ì–´ Top 50 ===\")\n",
        "print(top_neg_words)\n"
      ],
      "metadata": {
        "id": "m2JoW1A1iRum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "pDFJBZbijwxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "8nbXaiK5jy5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch sentencepiece tqdm\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "r2tYsBSbjHVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ë¶ˆìš©ì–´ ì¶”ê°€ë²„ì „ ì „ì²´ í‚¤ì›Œë“œ\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# 1ï¸âƒ£ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "df = pd.read_csv('/content/everland_reviews (1).csv')\n",
        "text_col = 'content'\n",
        "\n",
        "# 2ï¸âƒ£ KoBERT ëª¨ë¸ ë¡œë“œ\n",
        "MODEL_NAME = \"snunlp/KR-FinBert-SC\"  # ê°ì„±ë¶„ì„ìš© KoBERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# 3ï¸âƒ£ ê°ì • ì˜ˆì¸¡ í•¨ìˆ˜\n",
        "def predict_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    pred = torch.argmax(logits, dim=1).item()\n",
        "    return pred  # 0: ë¶€ì •, 1: ì¤‘ë¦½, 2: ê¸ì •\n",
        "\n",
        "# 4ï¸âƒ£ ê°ì„±ë¶„ì„ ì‹¤í–‰\n",
        "tqdm.pandas()\n",
        "df['sentiment'] = df[text_col].progress_apply(predict_sentiment)\n",
        "\n",
        "# 5ï¸âƒ£ ê°ì„± ë ˆì´ë¸” ë§¤í•‘\n",
        "sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
        "df['sentiment_label'] = df['sentiment'].map(sentiment_map)\n",
        "\n",
        "print(\"=== ğŸ¯ KoBERT ê°ì„± ë¶„í¬ ===\")\n",
        "print(df['sentiment_label'].value_counts())\n",
        "\n",
        "print(\"\\n=== ìƒ˜í”Œ ë¦¬ë·° ê°ì„± ì˜ˆì¸¡ ===\")\n",
        "print(df[[text_col, 'sentiment_label']].head(10))\n",
        "\n",
        "# 6ï¸âƒ£ ì „ì²´ ë¦¬ë·° í…ìŠ¤íŠ¸ ê²°í•©\n",
        "all_text = \" \".join(df[text_col].astype(str))\n",
        "all_text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', ' ', all_text)\n",
        "\n",
        "# 7ï¸âƒ£ ê¸´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ì •í™•ë„ ê°•í™” ë²„ì „)\n",
        "stopwords = set([\n",
        "    'ì€','ëŠ”','ì´','ê°€','ì„','ë¥¼','ë„','ì—','ì™€','ê³¼','ì˜','ë¡œ','ìœ¼ë¡œ','ì—ì„œ','ì—ê²Œ','ë³´ë‹¤','ìœ¼ë¡œì„œ',\n",
        "    'ë˜','ë˜í•œ','ê·¸ë¦¬ê³ ','í•˜ì§€ë§Œ','ê·¸ë˜ì„œ','ê·¸ëŸ¬ë‚˜','ë•Œë¬¸','ë•Œë¬¸ì—','ì´ëŸ°','ì €ëŸ°','ê·¸ëŸ°','ìš”','ë‹¤','ë“¯','ë“¯ì´',\n",
        "    'ë„ˆë¬´','ë§¤ìš°','ì •ë§','ì§„ì§œ','ì•„ì£¼','êµ‰ì¥íˆ','ì—„ì²­','ë§ì´','ë§ì•„ì„œ','ë§ê³ ','ë§ì€','ì¢‹ì€','ì¢‹ì•„ìš”','ì¢‹ì•˜ì–´ìš”',\n",
        "    'ìµœê³ ','ìµœê³ ì˜','ì—­ì‹œ','ìˆëŠ”','ìˆì–´ì„œ','ìˆê³ ','ê°™ì€','ê°™ì•„ìš”','ê°™ì´','ê·¸ëƒ¥','ì¢€','ì¡°ê¸ˆ','ì•½ê°„','ì •ë„',\n",
        "    'í•­ìƒ','ê°€ë”','ì–¸ì œë‚˜','ì •ë§ë¡œ','ê·¸ë ‡ë‹¤','ì´ë ‡ë‹¤','ã…‹ã…‹','ã…ã…','ã… ã… ','ã…œã…œ','ì™€','ìš°ì™€','í—','í•˜í•˜','ì•¼',\n",
        "    'ì•„','ìŒ','ì‘','ì—íœ´','íœ´','í•˜','ì–´íœ´','ì§±','ì‚¬ëŒ','ì‚¬ëŒì´','ì‚¬ëŒë“¤','ì—ë²„ëœë“œ','ì—ë²„ëœë“œëŠ”','ê³³','ê³³ì´',\n",
        "    'ê³³ë„','ì •ë§ë¡œ','ë˜ëŠ”','í˜¹ì€','ë‹¤ë…€ì™”ì–´ìš”','íƒ€ê³ ','íƒ”ì–´ìš”','ê°€ì„œ','ì™”ë‹¤','ì™”ë‹¤ê°€','í–ˆìŠµë‹ˆë‹¤','í–ˆì–´ìš”',\n",
        "    'ë³´ë‹ˆ','ë³´ë‹ˆê¹Œ','ì´ìš©','ì´ìš©í–ˆì–´ìš”','ê°€ê¸°','ê°”ë‹¤','ê°€ëŠ”','ê°€ì„œ','ì™”ì–´ìš”','ê°”ì–´ìš”','í–ˆì–´ìš”','ë´¤ì–´ìš”',\n",
        "    'ë´ì„œ','ë´¤ë‹¤','ë´¤ìŠµë‹ˆë‹¤','ì—ì„œ','ìœ¼ë¡œ','í•˜ì—¬','ë˜ì–´','ë˜ì–´ì„œ','í•´ìš”','ë„¤ìš”','ì–´ìš”','í–ˆë‹µë‹ˆë‹¤',\n",
        "    'ì‹¶ì–´ìš”','ì‹¶ì—ˆì–´ìš”','í•©ë‹ˆë‹¤','í–ˆë„¤ìš”','í–ˆêµ¬ìš”','ì´ë„¤ìš”','ì´ì—ˆì–´ìš”','ì˜€ì–´ìš”','ì´ì—ìš”','ë„¤ìš”','ì´ì—ìš”'\n",
        "])\n",
        "\n",
        "# 8ï¸âƒ£ ë¶ˆìš©ì–´ ì œê±° + ë‹¨ì–´ ë¶„ë¦¬\n",
        "filtered_words = [w for w in all_text.split() if w not in stopwords and len(w) > 1]\n",
        "word_freq = Counter(filtered_words)\n",
        "\n",
        "# 9ï¸âƒ£ Top 50 ë‹¨ì–´ ì¶”ì¶œ\n",
        "top50 = pd.DataFrame(word_freq.most_common(50), columns=['Word','Frequency'])\n",
        "print(\"\\n=== ğŸ·ï¸ ì „ì²´ ë¦¬ë·° ë‚´ ë‹¨ì–´ ë¹ˆë„ Top 50 (ì •í™•ë„ í–¥ìƒ ë²„ì „) ===\")\n",
        "print(top50)\n"
      ],
      "metadata": {
        "id": "A15Qi9w_kTK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# ğŸ§  KoBERT ì „ì²´ ë¦¬ë·° ê°ì„± ë¶„ì„ + ë‹¨ì–´ ë¹ˆë„ë¶„ì„ (ì •í™•ë„ ê°•í™” ë²„ì „)\n",
        "# ============================================\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# 1ï¸âƒ£ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "df = pd.read_csv('/content/everland_reviews (1).csv')\n",
        "text_col = 'content'   # ğŸ”¹ ë¦¬ë·° ì»¬ëŸ¼ëª… (í•„ìš”ì‹œ ë³€ê²½)\n",
        "\n",
        "# 2ï¸âƒ£ KoBERT ëª¨ë¸ ë¡œë“œ\n",
        "MODEL_NAME = \"snunlp/KR-FinBert-SC\"  # í•œêµ­ì–´ ê°ì • ë¶„ì„ìš© BERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# 3ï¸âƒ£ ê°ì • ì˜ˆì¸¡ í•¨ìˆ˜\n",
        "def predict_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    pred = torch.argmax(logits, dim=1).item()\n",
        "    return pred  # 0: ë¶€ì •, 1: ì¤‘ë¦½, 2: ê¸ì •\n",
        "\n",
        "# 4ï¸âƒ£ ê°ì„± ë¶„ì„ ì‹¤í–‰ (ì—ëŸ¬ ë°©ì§€ í¬í•¨)\n",
        "tqdm.pandas()\n",
        "df[text_col] = df[text_col].astype(str).fillna(\"\")   # âœ… ê²°ì¸¡ê°’ ë° ë¹„ë¬¸ì ì²˜ë¦¬\n",
        "df['sentiment'] = df[text_col].progress_apply(predict_sentiment)\n",
        "\n",
        "# 5ï¸âƒ£ ê°ì„± ë ˆì´ë¸” ë§¤í•‘\n",
        "sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
        "df['sentiment_label'] = df['sentiment'].map(sentiment_map)\n",
        "\n",
        "print(\"=== ğŸ¯ KoBERT ê°ì„± ë¶„í¬ ===\")\n",
        "print(df['sentiment_label'].value_counts())\n",
        "\n",
        "print(\"\\n=== ìƒ˜í”Œ ë¦¬ë·° ê°ì„± ì˜ˆì¸¡ ===\")\n",
        "print(df[[text_col, 'sentiment_label']].head(10))\n",
        "\n",
        "# ============================================\n",
        "# ğŸ”¤ Top 50 ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ (ì •í™•ë„ ê°•í™”)\n",
        "# ============================================\n",
        "\n",
        "# 6ï¸âƒ£ ì „ì²´ ë¦¬ë·° í…ìŠ¤íŠ¸ ê²°í•©\n",
        "all_text = \" \".join(df[text_col].astype(str))\n",
        "all_text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', ' ', all_text)\n",
        "\n",
        "# 7ï¸âƒ£ ê¸´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ì •í™•ë„ í–¥ìƒìš©)\n",
        "stopwords = set([\n",
        "   # ê¸°ë³¸ ì¡°ì‚¬, ì ‘ì†ì‚¬\n",
        "    'ì€','ëŠ”','ì´','ê°€','ì„','ë¥¼','ì—','ì™€','ê³¼','ì˜','ë¡œ','ìœ¼ë¡œ','ì—ì„œ','ì—ê²Œ','ë³´ë‹¤','ìœ¼ë¡œì„œ','ë˜','ë˜í•œ',\n",
        "    'ê·¸ë¦¬ê³ ','í•˜ì§€ë§Œ','ê·¸ë˜ì„œ','ê·¸ëŸ¬ë‚˜','ë•Œë¬¸','ë•Œë¬¸ì—','ì´ëŸ°','ì €ëŸ°','ê·¸ëŸ°','ìš”','ë‹¤','ë“¯','ë“¯ì´','ì•„ì§','ê·¸ëŸ°ë°',\n",
        "\n",
        "    # ê°íƒ„ì‚¬, ì¤‘ë¦½í˜• í‘œí˜„\n",
        "    'ë„ˆë¬´','ë§¤ìš°','ì •ë§','ì§„ì§œ','ì•„ì£¼','êµ‰ì¥íˆ','ì—„ì²­','ë§ì´','ë§ì•„ìš”','ë§ì•„ì„œ','ë§ê³ ','ë§ì€','ë§ë‹¤','ë§ì•˜ì–´ìš”',\n",
        "    'ì¢‹ì•„ìš”','ì¢‹ë„¤ìš”','ì¢‹ì•˜ì–´ìš”','ì¢‹ê³ ','ì¢‹ë‹¤','ì¢‹ì•˜ìŠµë‹ˆë‹¤','ì¢‹ìŠµë‹ˆë‹¤','ì¢‹ì€','ì¢‹ì€ê³³','ì¢‹ì€ê³³ì´ì—ìš”',\n",
        "    'ìµœê³ ','ìµœê³ ì˜','ì—­ì‹œ','ìˆì–´ìš”','ìˆë„¤ìš”','ìˆìŠµë‹ˆë‹¤','ìˆê³ ','ìˆì–´ì„œ','ìˆëŠ”','ê°™ì•„ìš”','ê°™ì´','ê°™ì€','ê°™ë„¤ìš”',\n",
        "    'í•©ë‹ˆë‹¤','í–ˆì–´ìš”','í–ˆë„¤ìš”','í–ˆêµ¬ìš”','í–ˆìŠµë‹ˆë‹¤','í–ˆë‹µë‹ˆë‹¤','í•´ìš”','ë¼ìš”','ë©ë‹ˆë‹¤','ë˜ë„¤ìš”','ë˜ìš”','ë˜ì–´ìš”',\n",
        "    'ë„¤ìš”','ìš”','ìŠµë‹ˆë‹¤','ì—ˆì–´ìš”','ì´ì—ˆì–´ìš”','ì˜€ì–´ìš”','ì—ìš”','ì´ì—ìš”','ë„¤ìš”','ì´ë„¤ìš”','ì˜€ë„¤ìš”','ì´ì—ˆë„¤ìš”',\n",
        "\n",
        "    # ë¶ˆí•„ìš” ë™ì‚¬, ë¶€ì‚¬\n",
        "    'ê·¸ëƒ¥','ì¢€','ì¡°ê¸ˆ','ì•½ê°„','ì •ë„','í•­ìƒ','ê°€ë”','ì–¸ì œë‚˜','ì •ë§ë¡œ','ê·¸ë ‡ë‹¤','ì´ë ‡ë‹¤','ê·¸ë¬ì–´ìš”','ì´ë¬ì–´ìš”',\n",
        "    'í–ˆì–´ìš”','í–ˆë„¤ìš”','í–ˆìŠµë‹ˆë‹¤','í–ˆë‹µë‹ˆë‹¤','í–ˆêµ¬ìš”','ë˜ì—ˆì–´ìš”','ë©ë‹ˆë‹¤','ë˜ì–´','ë˜ì–´ì„œ','ë˜ì–´ìˆì–´ìš”',\n",
        "\n",
        "    # ê°íƒ„ì‚¬, í‘œí˜„ì–´\n",
        "    'ã…‹ã…‹','ã…ã…','ã… ã… ','ã…œã…œ','í•˜í•˜','í—','ì™€','ìš°ì™€','ì•¼','ì•„','ìŒ','ì‘','íœ´','í•˜','ì—íœ´','ì–´íœ´','ì§±',\n",
        "\n",
        "    # ì¼ë°˜ ëª…ì‚¬í˜• ë¶ˆí•„ìš” ë‹¨ì–´\n",
        "    'ì‚¬ëŒ','ì‚¬ëŒì´','ì‚¬ëŒë“¤','ì—ë²„ëœë“œ','ì—ë²„ëœë“œëŠ”','ê³³','ê³³ì´','ê³³ë„','ì •ë§ë¡œ','ë˜ëŠ”','í˜¹ì€','ì‹œê°„','ì‹œê°„ì´',\n",
        "    'ê°€ì„œ','ì™”ì–´ìš”','ê°”ì–´ìš”','ì™”ë‹¤','ì™”ë‹¤ê°€','ê°€ìš”','ê°€ë©´','ê°€ê¸°','ê°”ë‹¤','ê°€ëŠ”','ê°€ì•¼','ë´ì„œ','ë´¤ì–´ìš”','ë´¤ìŠµë‹ˆë‹¤',\n",
        "    'ë´¤ë‹¤','ë³´ë‹ˆ','ë³´ë‹ˆê¹Œ','ì´ìš©','ì´ìš©í–ˆì–´ìš”','ì´ìš©í–ˆìŠµë‹ˆë‹¤','í•´ì„œ','í•´ì„œìš”','í•œë‹¤','í–ˆêµ¬ìš”','í•´ì„œìš”','í–ˆì–´ìš”',\n",
        "\n",
        "    # ì„œìˆ í˜• ì¢…ê²°ì–´ ì œê±°\n",
        "    'ê°™ì•„ìš”','ê°™ë„¤ìš”','ê°™ì€ë°','ê°™ìŠµë‹ˆë‹¤','í•©ë‹ˆë‹¤','í–ˆì–´ìš”','í–ˆë‹µë‹ˆë‹¤','í–ˆêµ¬ìš”','ë©ë‹ˆë‹¤','ë˜ë„¤ìš”','ë¼ìš”','ë˜ì–´ìš”',\n",
        "    'ë„¤ìš”','ì—ìš”','ì´ì—ìš”','ì˜€ì–´ìš”','ì´ì—ˆì–´ìš”','ë„¤ìš”','ì´ë„¤ìš”','ìŠµë‹ˆë‹¤','ìš”','ìˆì–´ìš”','ìˆë„¤ìš”','ìˆìŠµë‹ˆë‹¤'\n",
        "])\n",
        "\n",
        "# 8ï¸âƒ£ ë‹¨ì–´ í•„í„°ë§ ë° ë¹ˆë„ ê³„ì‚°\n",
        "filtered_words = [w for w in all_text.split() if w not in stopwords and len(w) > 1]\n",
        "word_freq = Counter(filtered_words)\n",
        "\n",
        "# 9ï¸âƒ£ ìƒìœ„ 50ê°œ ë‹¨ì–´ ì¶”ì¶œ\n",
        "top50 = pd.DataFrame(word_freq.most_common(50), columns=['Word','Frequency'])\n",
        "\n",
        "print(\"\\n=== ğŸ·ï¸ ì „ì²´ ë¦¬ë·° ë‚´ ë‹¨ì–´ ë¹ˆë„ Top 50 (ì •í™•ë„ í–¥ìƒ ë²„ì „) ===\")\n",
        "print(top50)\n",
        "\n",
        "# ============================================\n",
        "# âœ… ê²°ê³¼ ì €ì¥ (ì›í•  ì‹œ)\n",
        "# ============================================\n",
        "# df.to_csv('/content/everland_reviews_sentiment.csv', index=False)\n",
        "# top50.to_csv('/content/top50_words.csv', index=False)\n"
      ],
      "metadata": {
        "id": "v6JfJ4dZpMEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KoBERT ê¸°ë°˜ â€œë¶€ì • ë¦¬ë·°ë§Œâ€ ë¶„ì„ (Top 50 ë‹¨ì–´ í¬í•¨)\n",
        "\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# 1ï¸âƒ£ ë¶€ì • ë¦¬ë·°ë§Œ ì¶”ì¶œ\n",
        "neg_df = df[df['sentiment_label'] == 'negative'].copy()\n",
        "\n",
        "# 2ï¸âƒ£ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ì´ì „ê³¼ ë™ì¼)\n",
        "stopwords = set([\n",
        "    # ê¸°ë³¸ ì¡°ì‚¬, ì ‘ì†ì‚¬\n",
        "    'ì€','ëŠ”','ì´','ê°€','ì„','ë¥¼','ì—','ì™€','ê³¼','ì˜','ë¡œ','ìœ¼ë¡œ','ì—ì„œ','ì—ê²Œ','ë³´ë‹¤','ìœ¼ë¡œì„œ','ë˜','ë˜í•œ',\n",
        "    'ê·¸ë¦¬ê³ ','í•˜ì§€ë§Œ','ê·¸ë˜ì„œ','ê·¸ëŸ¬ë‚˜','ë•Œë¬¸','ë•Œë¬¸ì—','ì´ëŸ°','ì €ëŸ°','ê·¸ëŸ°','ìš”','ë‹¤','ë“¯','ë“¯ì´','ì•„ì§','ê·¸ëŸ°ë°',\n",
        "\n",
        "    # ê°íƒ„ì‚¬, ì¤‘ë¦½í˜• í‘œí˜„\n",
        "    'ë„ˆë¬´','ë§¤ìš°','ì •ë§','ì§„ì§œ','ì•„ì£¼','êµ‰ì¥íˆ','ì—„ì²­','ë§ì´','ë§ì•„ìš”','ë§ì•„ì„œ','ë§ê³ ','ë§ì€','ë§ë‹¤','ë§ì•˜ì–´ìš”',\n",
        "    'ì¢‹ì•„ìš”','ì¢‹ë„¤ìš”','ì¢‹ì•˜ì–´ìš”','ì¢‹ê³ ','ì¢‹ë‹¤','ì¢‹ì•˜ìŠµë‹ˆë‹¤','ì¢‹ìŠµë‹ˆë‹¤','ì¢‹ì€','ì¢‹ì€ê³³','ì¢‹ì€ê³³ì´ì—ìš”',\n",
        "    'ìµœê³ ','ìµœê³ ì˜','ì—­ì‹œ','ìˆì–´ìš”','ìˆë„¤ìš”','ìˆìŠµë‹ˆë‹¤','ìˆê³ ','ìˆì–´ì„œ','ìˆëŠ”','ê°™ì•„ìš”','ê°™ì´','ê°™ì€','ê°™ë„¤ìš”',\n",
        "    'í•©ë‹ˆë‹¤','í–ˆì–´ìš”','í–ˆë„¤ìš”','í–ˆêµ¬ìš”','í–ˆìŠµë‹ˆë‹¤','í–ˆë‹µë‹ˆë‹¤','í•´ìš”','ë¼ìš”','ë©ë‹ˆë‹¤','ë˜ë„¤ìš”','ë˜ìš”','ë˜ì–´ìš”',\n",
        "    'ë„¤ìš”','ìš”','ìŠµë‹ˆë‹¤','ì—ˆì–´ìš”','ì´ì—ˆì–´ìš”','ì˜€ì–´ìš”','ì—ìš”','ì´ì—ìš”','ë„¤ìš”','ì´ë„¤ìš”','ì˜€ë„¤ìš”','ì´ì—ˆë„¤ìš”',\n",
        "\n",
        "    # ë¶ˆí•„ìš” ë™ì‚¬, ë¶€ì‚¬\n",
        "    'ê·¸ëƒ¥','ì¢€','ì¡°ê¸ˆ','ì•½ê°„','ì •ë„','í•­ìƒ','ê°€ë”','ì–¸ì œë‚˜','ì •ë§ë¡œ','ê·¸ë ‡ë‹¤','ì´ë ‡ë‹¤','ê·¸ë¬ì–´ìš”','ì´ë¬ì–´ìš”',\n",
        "    'í–ˆì–´ìš”','í–ˆë„¤ìš”','í–ˆìŠµë‹ˆë‹¤','í–ˆë‹µë‹ˆë‹¤','í–ˆêµ¬ìš”','ë˜ì—ˆì–´ìš”','ë©ë‹ˆë‹¤','ë˜ì–´','ë˜ì–´ì„œ','ë˜ì–´ìˆì–´ìš”',\n",
        "\n",
        "    # ê°íƒ„ì‚¬, í‘œí˜„ì–´\n",
        "    'ã…‹ã…‹','ã…ã…','ã… ã… ','ã…œã…œ','í•˜í•˜','í—','ì™€','ìš°ì™€','ì•¼','ì•„','ìŒ','ì‘','íœ´','í•˜','ì—íœ´','ì–´íœ´','ì§±',\n",
        "\n",
        "    # ì¼ë°˜ ëª…ì‚¬í˜• ë¶ˆí•„ìš” ë‹¨ì–´\n",
        "    'ì‚¬ëŒ','ì‚¬ëŒì´','ì‚¬ëŒë“¤','ì—ë²„ëœë“œ','ì—ë²„ëœë“œëŠ”','ê³³','ê³³ì´','ê³³ë„','ì •ë§ë¡œ','ë˜ëŠ”','í˜¹ì€','ì‹œê°„','ì‹œê°„ì´',\n",
        "    'ê°€ì„œ','ì™”ì–´ìš”','ê°”ì–´ìš”','ì™”ë‹¤','ì™”ë‹¤ê°€','ê°€ìš”','ê°€ë©´','ê°€ê¸°','ê°”ë‹¤','ê°€ëŠ”','ê°€ì•¼','ë´ì„œ','ë´¤ì–´ìš”','ë´¤ìŠµë‹ˆë‹¤',\n",
        "    'ë´¤ë‹¤','ë³´ë‹ˆ','ë³´ë‹ˆê¹Œ','ì´ìš©','ì´ìš©í–ˆì–´ìš”','ì´ìš©í–ˆìŠµë‹ˆë‹¤','í•´ì„œ','í•´ì„œìš”','í•œë‹¤','í–ˆêµ¬ìš”','í•´ì„œìš”','í–ˆì–´ìš”',\n",
        "\n",
        "    # ì„œìˆ í˜• ì¢…ê²°ì–´ ì œê±°\n",
        "    'ê°™ì•„ìš”','ê°™ë„¤ìš”','ê°™ì€ë°','ê°™ìŠµë‹ˆë‹¤','í•©ë‹ˆë‹¤','í–ˆì–´ìš”','í–ˆë‹µë‹ˆë‹¤','í–ˆêµ¬ìš”','ë©ë‹ˆë‹¤','ë˜ë„¤ìš”','ë¼ìš”','ë˜ì–´ìš”',\n",
        "    'ë„¤ìš”','ì—ìš”','ì´ì—ìš”','ì˜€ì–´ìš”','ì´ì—ˆì–´ìš”','ë„¤ìš”','ì´ë„¤ìš”','ìŠµë‹ˆë‹¤','ìš”','ìˆì–´ìš”','ìˆë„¤ìš”','ìˆìŠµë‹ˆë‹¤'\n",
        "])\n",
        "\n",
        "# 3ï¸âƒ£ í™•ì¥ í…Œë§ˆ í‚¤ì›Œë“œ\n",
        "theme_keywords = {\n",
        "    'Service': ['ë¶ˆì¹œì ˆ','ë¶ˆë§Œ','ì‘ëŒ€','íƒœë„','ì§ì›','ì„œë¹„ìŠ¤','ì†Œí†µ','ì•ˆë‚´','ê³ ê°ì„¼í„°'],\n",
        "    'Facility': ['ë”ëŸ½','ëƒ„ìƒˆ','ë¶ˆí¸','ê³ ì¥','ê´€ë¦¬','ì²­ê²°','ì‹œì„¤','ì •ë¹„','ë…¸í›„','ìœ„í—˜','ìœ„ìƒ'],\n",
        "    'Price': ['ë¹„ì‹¸','ë¹„ìŒˆ','ê°€ê²©','ì¿ í°','í• ì¸ì—†ìŒ','ì…ì¥ë£Œ','ê°€ì„±ë¹„','ë¹„ìš©','ë¹„íš¨ìœ¨','ëˆ'],\n",
        "    'Waiting': ['ëŒ€ê¸°','ì¤„','ì›¨ì´íŒ…','ë¶ë¹”','í˜¼ì¡','ê¸°ë‹¤ë¦¼','ëŠ¦ìŒ','ì§€ì—°','ì •ì²´','ê¸¸ë‹¤','ì‹œê°„'],\n",
        "    'Attractions': ['ê³ ì¥','ë©ˆì¶¤','ìŠ¤ë¦´ì—†','ì§€ë£¨','ë³„ë¡œ','ì‹¤ë§','ì¬ë¯¸ì—†','ìš´íœ´','ì§§ìŒ','ë¶ˆì¾Œ','ì§€ì €ë¶„']\n",
        "}\n",
        "\n",
        "# 4ï¸âƒ£ ë¶€ì • ë¦¬ë·° í…ìŠ¤íŠ¸ ì „ì²´ ê²°í•©\n",
        "all_text = \" \".join(neg_df[text_col].astype(str))\n",
        "all_text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', ' ', all_text)\n",
        "words = [w for w in all_text.split() if w not in stopwords and len(w) > 1]\n",
        "word_freq = Counter(words)\n",
        "\n",
        "# 5ï¸âƒ£ í…Œë§ˆë³„ ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°\n",
        "theme_counts = {}\n",
        "for theme, keywords in theme_keywords.items():\n",
        "    count = sum(word_freq[k] for k in keywords if k in word_freq)\n",
        "    theme_counts[theme] = count\n",
        "\n",
        "theme_df = pd.DataFrame(list(theme_counts.items()), columns=['Theme','NegativeKeywordCount']).sort_values('NegativeKeywordCount', ascending=False)\n",
        "top50 = pd.DataFrame(word_freq.most_common(50), columns=['Word','Frequency'])\n",
        "\n",
        "print(\"=== ğŸ’¢ KoBERT ê¸°ë°˜ ë¶€ì • ë¦¬ë·° í…Œë§ˆë³„ í‚¤ì›Œë“œ ë“±ì¥ íšŸìˆ˜ ===\")\n",
        "print(theme_df)\n",
        "print(\"\\n=== ğŸ’¢ KoBERT ê¸°ë°˜ ë¶€ì • ë¦¬ë·° ë‹¨ì–´ Top 50 ===\")\n",
        "print(top50)\n"
      ],
      "metadata": {
        "id": "KKr0nHUfjV_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# ğŸ§  KoELECTRA ë¦¬ë·° ê°ì„± ë¶„ì„ + í…Œë§ˆë³„ ë‹¨ì–´ë¶„ì„ ì™„ì „íŒ\n",
        "# ============================================\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ============================================\n",
        "# 1ï¸âƒ£ ë°ì´í„° ë¡œë“œ\n",
        "# ============================================\n",
        "df = pd.read_csv('/content/everland_reviews (1).csv')\n",
        "text_col = 'content'\n",
        "\n",
        "# ============================================\n",
        "# 2ï¸âƒ£ ëª¨ë¸ ë¡œë“œ (KoELECTRA ëŒ€ì²´ ê³µê°œëª¨ë¸)\n",
        "# ============================================\n",
        "MODEL_NAME = \"beomi/KcELECTRA-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 3ï¸âƒ£ ê°ì„± ì˜ˆì¸¡ í•¨ìˆ˜\n",
        "# ============================================\n",
        "def predict_sentiment(text):\n",
        "    if not isinstance(text, str) or text.strip() == \"\":\n",
        "        return 1  # ì¤‘ë¦½ ì²˜ë¦¬\n",
        "    sentences = re.split('[.!?\\\\n]', text)\n",
        "    preds = []\n",
        "    for s in sentences:\n",
        "        s = s.strip()\n",
        "        if len(s) < 2:\n",
        "            continue\n",
        "        inputs = tokenizer(s, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        pred = torch.argmax(logits, dim=1).item()\n",
        "        preds.append(pred)\n",
        "    if len(preds) == 0:\n",
        "        return 1\n",
        "    return round(sum(preds) / len(preds))  # 0=ë¶€ì •, 1=ê¸ì •\n",
        "\n",
        "# ============================================\n",
        "# 4ï¸âƒ£ ê°ì„± ë¶„ì„ ì‹¤í–‰\n",
        "# ============================================\n",
        "tqdm.pandas()\n",
        "df[text_col] = df[text_col].astype(str).fillna(\"\")\n",
        "df['sentiment'] = df[text_col].progress_apply(predict_sentiment)\n",
        "\n",
        "# 5ï¸âƒ£ ê°ì„± ë¼ë²¨ ë³€í™˜\n",
        "sentiment_map = {0: 'negative', 1: 'positive'}\n",
        "df['sentiment_label'] = df['sentiment'].map(sentiment_map)\n",
        "\n",
        "print(\"=== ğŸ¯ ê°ì„± ë¶„í¬ ===\")\n",
        "print(df['sentiment_label'].value_counts())\n",
        "\n",
        "# ============================================\n",
        "# 6ï¸âƒ£ ê°ì„± ë¶„í¬ ì‹œê°í™”\n",
        "# ============================================\n",
        "plt.figure(figsize=(5,4))\n",
        "df['sentiment_label'].value_counts().plot(kind='bar', color=['red','green'])\n",
        "plt.title('Everland Reviews Sentiment Distribution')\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# 7ï¸âƒ£ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ê°•í™” ë²„ì „)\n",
        "# ============================================\n",
        "stopwords = set([\n",
        "    'ì€','ëŠ”','ì´','ê°€','ì„','ë¥¼','ì—','ì™€','ê³¼','ì˜','ë¡œ','ìœ¼ë¡œ','ì—ì„œ','ì—ê²Œ','ë³´ë‹¤','ìœ¼ë¡œì„œ',\n",
        "    'ë˜','ë˜í•œ','ê·¸ë¦¬ê³ ','í•˜ì§€ë§Œ','ê·¸ë˜ì„œ','ê·¸ëŸ¬ë‚˜','ë•Œë¬¸','ë•Œë¬¸ì—','ì´ëŸ°','ì €ëŸ°','ê·¸ëŸ°','ìš”','ë‹¤','ë“¯','ë“¯ì´',\n",
        "    'ë„ˆë¬´','ë§¤ìš°','ì •ë§','ì§„ì§œ','ì•„ì£¼','êµ‰ì¥íˆ','ì—„ì²­','ë§ì´','ë§ì•„ìš”','ë§ì•„ì„œ','ë§ê³ ','ë§ì€','ë§ë‹¤','ì¢‹ì•„ìš”','ì¢‹ë„¤ìš”',\n",
        "    'ì¢‹ì•˜ì–´ìš”','ì¢‹ê³ ','ì¢‹ë‹¤','ì¢‹ì•˜ìŠµë‹ˆë‹¤','ì¢‹ìŠµë‹ˆë‹¤','ì¢‹ì€','ì¢‹ì€ê³³','ìµœê³ ','ìµœê³ ì˜','ì—­ì‹œ','ìˆì–´ìš”','ìˆë„¤ìš”','ìˆìŠµë‹ˆë‹¤',\n",
        "    'ìˆê³ ','ìˆì–´ì„œ','ìˆëŠ”','ê°™ì•„ìš”','ê°™ì´','ê°™ì€','ê°™ë„¤ìš”','í•©ë‹ˆë‹¤','í–ˆì–´ìš”','í–ˆë„¤ìš”','í–ˆêµ¬ìš”','í–ˆìŠµë‹ˆë‹¤','í–ˆë‹µë‹ˆë‹¤',\n",
        "    'í•´ìš”','ë¼ìš”','ë©ë‹ˆë‹¤','ë˜ë„¤ìš”','ë˜ìš”','ë˜ì–´ìš”','ë„¤ìš”','ìš”','ìŠµë‹ˆë‹¤','ì—ˆì–´ìš”','ì´ì—ˆì–´ìš”','ì˜€ì–´ìš”','ì—ìš”','ì´ì—ìš”',\n",
        "    'ë„¤ìš”','ì´ë„¤ìš”','ì˜€ë„¤ìš”','ì´ì—ˆë„¤ìš”','ê·¸ëƒ¥','ì¢€','ì¡°ê¸ˆ','ì•½ê°„','ì •ë„','í•­ìƒ','ê°€ë”','ì–¸ì œë‚˜','ì •ë§ë¡œ','ê·¸ë ‡ë‹¤','ì´ë ‡ë‹¤',\n",
        "    'ê·¸ë¬ì–´ìš”','ì´ë¬ì–´ìš”','í–ˆì–´ìš”','í–ˆë„¤ìš”','í–ˆìŠµë‹ˆë‹¤','í–ˆë‹µë‹ˆë‹¤','ë˜ì—ˆì–´ìš”','ë˜ì–´','ë˜ì–´ì„œ','ë˜ì–´ìˆì–´ìš”','ã…‹ã…‹','ã…ã…',\n",
        "    'ã… ã… ','ã…œã…œ','í•˜í•˜','í—','ì™€','ìš°ì™€','ì•¼','ì•„','ìŒ','ì‘','íœ´','í•˜','ì—íœ´','ì–´íœ´','ì§±','ì‚¬ëŒ','ì‚¬ëŒì´','ì‚¬ëŒë“¤',\n",
        "    'ì—ë²„ëœë“œ','ì—ë²„ëœë“œëŠ”','ê³³','ê³³ì´','ê³³ë„','ì •ë§ë¡œ','ë˜ëŠ”','í˜¹ì€','ì‹œê°„','ì‹œê°„ì´','ê°€ì„œ','ì™”ì–´ìš”','ê°”ì–´ìš”','ì™”ë‹¤',\n",
        "    'ì™”ë‹¤ê°€','ê°€ìš”','ê°€ë©´','ê°€ê¸°','ê°”ë‹¤','ê°€ëŠ”','ê°€ì•¼','ë´ì„œ','ë´¤ì–´ìš”','ë´¤ìŠµë‹ˆë‹¤','ë´¤ë‹¤','ë³´ë‹ˆ','ë³´ë‹ˆê¹Œ','ì´ìš©',\n",
        "    'ì´ìš©í–ˆì–´ìš”','ì´ìš©í–ˆìŠµë‹ˆë‹¤','í•´ì„œ','í•´ì„œìš”','í•œë‹¤','í–ˆêµ¬ìš”','í•´ì„œìš”','í–ˆì–´ìš”','ê°™ìŠµë‹ˆë‹¤','í•©ë‹ˆë‹¤','í–ˆì–´ìš”','ë©ë‹ˆë‹¤',\n",
        "    'ë˜ë„¤ìš”','ë¼ìš”','ë˜ì–´ìš”','ë„¤ìš”','ì—ìš”','ì´ì—ìš”','ì˜€ì–´ìš”','ì´ì—ˆì–´ìš”','ë„¤ìš”','ì´ë„¤ìš”','ìŠµë‹ˆë‹¤','ìš”','ìˆì–´ìš”','ìˆë„¤ìš”',\n",
        "    'ìˆìŠµë‹ˆë‹¤'\n",
        "])\n",
        "\n",
        "# ============================================\n",
        "# 8ï¸âƒ£ ì „ì²´ ë¦¬ë·° ë‹¨ì–´ ë¹ˆë„ Top 50\n",
        "# ============================================\n",
        "all_text = \" \".join(df[text_col].astype(str))\n",
        "all_text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', ' ', all_text)\n",
        "filtered_words = [w for w in all_text.split() if w not in stopwords and len(w) > 1]\n",
        "word_freq = Counter(filtered_words)\n",
        "top50 = pd.DataFrame(word_freq.most_common(50), columns=['Word','Frequency'])\n",
        "print(\"\\n=== ğŸ·ï¸ ì „ì²´ ë¦¬ë·° Top 50 ë‹¨ì–´ ===\")\n",
        "print(top50)\n",
        "\n",
        "# ============================================\n",
        "# 9ï¸âƒ£ ë¶€ì • ë¦¬ë·°ë§Œ ì¶”ì¶œ â†’ í…Œë§ˆë³„ ë¶„ì„\n",
        "# ============================================\n",
        "neg_df = df[df['sentiment_label'] == 'negative']\n",
        "neg_text = \" \".join(neg_df[text_col].astype(str))\n",
        "neg_text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', ' ', neg_text)\n",
        "\n",
        "filtered_neg_words = [w for w in neg_text.split() if w not in stopwords and len(w) > 1]\n",
        "neg_freq = Counter(filtered_neg_words)\n",
        "top50_neg = pd.DataFrame(neg_freq.most_common(50), columns=['Word','Frequency'])\n",
        "print(\"\\n=== ğŸ”´ ë¶€ì • ë¦¬ë·° ë‚´ Top 50 ë‹¨ì–´ ===\")\n",
        "print(top50_neg)\n",
        "# ğŸ”´ ë¶€ì • ë¦¬ë·° ë‚´ ê°ì •ì–´ ì¤‘ì‹¬ ë‹¨ì–´ ì¶”ì¶œ\n",
        "negative_words = [\n",
        "    'ë³„ë¡œ','ë¶ˆì¹œì ˆ','ë¹„ì‹¸','ë”ì›Œ','ì¶¥','ì§€ë£¨','ì˜¤ë˜','ê¸°ë‹¤','í˜ë“¤','ë§‰íˆ','ëŠ¦','ë¶ˆí¸','í˜¼ì¡',\n",
        "    'ë”ëŸ½','ëƒ„ìƒˆ','ë³µì¡','ê³ ì¥','ì—†','ë¶€ì¡±','ì•„ì‰¬','ì‹¤ë§','ì§œì¦','ë¶ˆì¾Œ','ë‚¡','ì‹œë„ëŸ½','í•œì°¸'\n",
        "]\n",
        "\n",
        "neg_filtered = [w for w in filtered_neg_words if any(n in w for n in negative_words)]\n",
        "neg_freq_sentiment = Counter(neg_filtered)\n",
        "top50_neg_sentiment = pd.DataFrame(neg_freq_sentiment.most_common(50), columns=['Word','Frequency'])\n",
        "print(\"\\n=== ğŸ”´ ë¶€ì • ê°ì •ì–´ ì¤‘ì‹¬ Top 50 ===\")\n",
        "print(top50_neg_sentiment)\n",
        "\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# ğŸ”Ÿ í…Œë§ˆ í‚¤ì›Œë“œ ë¶„ë¥˜\n",
        "# ============================================\n",
        "theme_keywords = {\n",
        "    'Service': ['ì§ì›','ì¹œì ˆ','ë¶ˆì¹œì ˆ','ì‘ëŒ€','ì„œë¹„ìŠ¤','íƒœë„','ì„¤ëª…','ì§ì›ì´','ëŒ€ì‘'],\n",
        "    'Facility': ['í™”ì¥ì‹¤','ì²­ê²°','ê´€ë¦¬','ê¹¨ë—','ì •ë¹„','ì‹œì„¤','ìœ„ìƒ','ëƒ„ìƒˆ','ìƒíƒœ'],\n",
        "    'Price': ['ê°€ê²©','ë¹„ì‹¸','ì¿ í°','í• ì¸','ì…ì¥ë£Œ','ê°€ì„±ë¹„','ë¹„ìš©','ëˆ'],\n",
        "    'Waiting': ['ëŒ€ê¸°','ì¤„','ì›¨ì´íŒ…','ë¶ë¹”','ê¸°ë‹¤ë¦¼','í˜¼ì¡','ì§€ì—°','ì‹œê°„'],\n",
        "    'Attractions': ['ë†€ì´ê¸°êµ¬','í¼ë ˆì´ë“œ','ê³µì—°','ìŠ¤ë¦´','ì¬ë°Œ','ì²´í—˜','ì´ë²¤íŠ¸','íƒí—˜']\n",
        "}\n",
        "\n",
        "theme_counts = {}\n",
        "for theme, words in theme_keywords.items():\n",
        "    count = sum(neg_text.count(w) for w in words)\n",
        "    theme_counts[theme] = count\n",
        "\n",
        "theme_df = pd.DataFrame(theme_counts.items(), columns=['Theme','Frequency']).sort_values(by='Frequency', ascending=False)\n",
        "\n",
        "print(\"\\n=== ğŸ“Š ë¶€ì • ë¦¬ë·° í…Œë§ˆë³„ ë¹ˆë„ ===\")\n",
        "print(theme_df)\n",
        "\n",
        "# ì‹œê°í™”\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(theme_df['Theme'], theme_df['Frequency'], color='salmon')\n",
        "plt.title('Negative Review Themes')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WZJ27bxskU6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# ğŸ§  KoELECTRA ë¦¬ë·° ê°ì„± ë¶„ì„ + í…Œë§ˆë³„ ë‹¨ì–´ë¶„ì„ ì™„ì „íŒ\n",
        "# ============================================\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ============================================\n",
        "# 1ï¸âƒ£ ë°ì´í„° ë¡œë“œ\n",
        "# ============================================\n",
        "df = pd.read_csv('/content/everland_reviews (1).csv')\n",
        "text_col = 'content'\n",
        "\n",
        "# ============================================\n",
        "# 2ï¸âƒ£ ëª¨ë¸ ë¡œë“œ (ì˜ë¬¸ ê°ì •ë¶„ì„ìš© ê³µê°œ ëª¨ë¸)\n",
        "# ============================================\n",
        "!pip install googletrans==4.0.0-rc1 > /dev/null\n",
        "\n",
        "from googletrans import Translator\n",
        "translator = Translator()\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from scipy.special import softmax\n",
        "\n",
        "MODEL_NAME = \"distilbert-base-uncased-finetuned-sst-2-english\"  # âœ… ê³µê°œ, ê°ì •ë¶„ì„ ì „ìš©\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# ============================================\n",
        "# 3ï¸âƒ£ ê°ì„± ì˜ˆì¸¡ í•¨ìˆ˜ (ë²ˆì—­ + ì˜ˆì¸¡)\n",
        "# ============================================\n",
        "def predict_sentiment(text):\n",
        "    if not isinstance(text, str) or text.strip() == \"\":\n",
        "        return 1  # ì¤‘ë¦½ ì²˜ë¦¬\n",
        "\n",
        "    try:\n",
        "        # âœ… í•œêµ­ì–´ â†’ ì˜ì–´ ë²ˆì—­\n",
        "        translated = translator.translate(text, src='ko', dest='en').text\n",
        "    except:\n",
        "        translated = text  # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ê·¸ëŒ€ë¡œ\n",
        "\n",
        "    inputs = tokenizer(translated, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    scores = softmax(outputs.logits.cpu().numpy()[0])\n",
        "    return 1 if scores[1] > scores[0] else 0  # 1=ê¸ì •, 0=ë¶€ì •\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 4ï¸âƒ£ ê°ì„± ë¶„ì„ ì‹¤í–‰\n",
        "# ============================================\n",
        "tqdm.pandas()\n",
        "df[text_col] = df[text_col].astype(str).fillna(\"\")\n",
        "df['sentiment'] = df[text_col].progress_apply(predict_sentiment)\n",
        "\n",
        "# 5ï¸âƒ£ ê°ì„± ë¼ë²¨ ë³€í™˜\n",
        "sentiment_map = {0: 'negative', 1: 'positive'}\n",
        "df['sentiment_label'] = df['sentiment'].map(sentiment_map)\n",
        "\n",
        "print(\"=== ğŸ¯ ê°ì„± ë¶„í¬ ===\")\n",
        "print(df['sentiment_label'].value_counts())\n",
        "\n",
        "# ============================================\n",
        "# 6ï¸âƒ£ ê°ì„± ë¶„í¬ ì‹œê°í™”\n",
        "# ============================================\n",
        "plt.figure(figsize=(5,4))\n",
        "df['sentiment_label'].value_counts().plot(kind='bar', color=['red','green'])\n",
        "plt.title('Everland Reviews Sentiment Distribution')\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# 7ï¸âƒ£ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ê°•í™” ë²„ì „)\n",
        "# ============================================\n",
        "# ============================================\n",
        "# ğŸ§¹ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ìµœì¢… ê°•í™” ë²„ì „)\n",
        "# ============================================\n",
        "stopwords = set([\n",
        "    # ì¡°ì‚¬Â·ì ‘ì†ì‚¬Â·ì¡°ë™ì‚¬\n",
        "    'ì€','ëŠ”','ì´','ê°€','ì„','ë¥¼','ì—','ì™€','ê³¼','ì˜','ë¡œ','ìœ¼ë¡œ','ì—ì„œ','ì—ê²Œ','ë³´ë‹¤','ìœ¼ë¡œì„œ',\n",
        "    'ë˜','ë˜í•œ','ê·¸ë¦¬ê³ ','í•˜ì§€ë§Œ','ê·¸ë˜ì„œ','ê·¸ëŸ¬ë‚˜','ë•Œë¬¸','ë•Œë¬¸ì—','ì´ëŸ°','ì €ëŸ°','ê·¸ëŸ°','ìš”','ë‹¤','ë“¯','ë“¯ì´',\n",
        "\n",
        "    # ê°íƒ„ì‚¬Â·ì¼ìƒí˜• ë¶€ì‚¬\n",
        "    'ë„ˆë¬´','ë§¤ìš°','ì •ë§','ì§„ì§œ','ì•„ì£¼','êµ‰ì¥íˆ','ì—„ì²­','ë§ì´','í•­ìƒ','ê°€ë”','ì–¸ì œë‚˜','ì •ë§ë¡œ',\n",
        "\n",
        "    # ê¸ì •Â·ê°•ì¡° í‘œí˜„ (ê°ì •ë¶„ì„ ì™œê³¡ ë°©ì§€ìš©)\n",
        "    'ì¢‹ì•„ìš”','ì¢‹ë„¤ìš”','ì¢‹ì•˜ì–´ìš”','ì¢‹ê³ ','ì¢‹ë‹¤','ì¢‹ì•˜ìŠµë‹ˆë‹¤','ì¢‹ìŠµë‹ˆë‹¤','ì¢‹ì€','ì¦ê±°ìš´','ì¦ê²ë‹¤',\n",
        "    'ìµœê³ ','ìµœê³ ì˜','ì—­ì‹œ','í™˜ìƒì˜','ì•„ë¦„ë‹µë‹¤','ë©‹ì ¸ìš”','ì˜ˆì˜ë‹¤','ì´ì˜ë‹¤','ì¬ë°Œë‹¤','ì¬ë¯¸ìˆë‹¤','ì¬ë°Œì–´ìš”',\n",
        "\n",
        "    # ì„œìˆ í˜• ì¢…ê²°ì–´Â·ì¡°ì‚¬\n",
        "    'ìˆì–´ìš”','ìˆë„¤ìš”','ìˆìŠµë‹ˆë‹¤','ìˆê³ ','ìˆì–´ì„œ','ìˆëŠ”','ìˆë‹¤','ë˜ì–´ìš”','ë©ë‹ˆë‹¤','ë˜ë„¤ìš”','ë¼ìš”',\n",
        "    'í•©ë‹ˆë‹¤','í–ˆì–´ìš”','í–ˆë„¤ìš”','í–ˆêµ¬ìš”','í–ˆìŠµë‹ˆë‹¤','í–ˆë‹µë‹ˆë‹¤','í•˜ë„¤ìš”','í•´ìš”','í•œë‹¤ê³ ','í•œë‹¤','í•˜ê³ ','í•˜ëŠ”','í–ˆì„',\n",
        "    'ì˜€ì–´ìš”','ì´ì—ˆì–´ìš”','ì—ìš”','ì´ì—ìš”','ë„¤ìš”','ì´ë„¤ìš”','ì˜€ìŠµë‹ˆë‹¤','í–ˆìŠµë‹ˆë‹¤','ì˜€ë„¤ìš”','ì´ì—ˆë„¤ìš”','í–ˆë„¤ìš”','í–ˆì–´ìš”',\n",
        "\n",
        "    # ê°íƒ„ì‚¬, ëŒ€í™”ì²´\n",
        "    'ã…‹ã…‹','ã…ã…','ã… ã… ','ã…œã…œ','í•˜í•˜','í—','ì™€','ìš°ì™€','ì•¼','ì•„','ìŒ','ì‘','íœ´','í•˜','ì—íœ´','ì–´íœ´','ì§±',\n",
        "\n",
        "    # ì¤‘ë¦½ ëª…ì‚¬Â·ë¶ˆí•„ìš” ë‹¨ì–´ (ì˜ë¯¸ ì—†ëŠ” ë¬¸ë§¥ì–´)\n",
        "    'ì‚¬ëŒ','ì‚¬ëŒì´','ì‚¬ëŒë“¤','ì‚¬ëŒë„','ì‚¬ëŒì€','ì‚¬ëŒë“¤ì´',\n",
        "    'ê·¸ë˜ë„','ìŠ¤ë§ˆíŠ¸','ê°”ëŠ”ë°','í•¨ê»˜','ë‹¤ë§Œ','ë‚˜ë¼','ê·¼ë°','íŠ¹íˆ','ê±°ì˜','ë‹¤ë¥¸','ê·¸ëŸ°ì§€','ì¶”ì²œ','ë¯¸ë¦¬',\n",
        "    'ì—†ëŠ”','ì—†ë„¤ìš”','ì—†ì–´ìš”','ì—†ê³ ','ì—†ìœ¼ë©°','ì—†ì—ˆì–´ìš”','ì—†ì§€ë§Œ','ì—†ë„¤ìš”','ì—†ìŒ','ì—†ì´','ì—†ìŠµë‹ˆë‹¤',\n",
        "    'ë†€ì´ê¸°êµ¬','ë†€ì´ê¸°êµ¬ë¥¼','ë†€ì´ê¸°êµ¬ê°€','ë†€ì´ê¸°êµ¬ëŠ”','ë†€ì´ê³µì›','ë†€ì´ë™ì‚°',\n",
        "    'íƒ€ê³ ','íƒ€ë‹¤','ë³´ê³ ','ë´¤ì–´ìš”','ë´¤ë‹¤','ë³´ë‹ˆ','ë³´ë‹ˆê¹Œ','ë´¤ìŠµë‹ˆë‹¤','ë´¤ë„¤ìš”',\n",
        "    'í‰ì¼','í‰ì¼ì—','ì£¼ë§','ì˜¤ëœë§Œì—','ì•„ì´ë“¤ì´','ì•„ì´ë“¤ê³¼','ì•„ì´','ì•„ì´ë“¤','ì•„ì´ë‘','ì•„ì´ì™€',\n",
        "    'ì‚¬íŒŒë¦¬','ë¡œìŠ¤íŠ¸ë°¸ë¦¬','ë¶ˆê½ƒë†€ì´','í• ë¡œìœˆ','í¼ë ˆì´ë“œ','ìŠ¤ë§ˆíŠ¸','ë‚˜ë¼','ëŒ€ê¸°','ì¤„ì„œê¸°',\n",
        "    'ê°™ì•„ìš”','ê°™ì´','ê°™ì€','ê°™ë„¤ìš”','ê°™ìŠµë‹ˆë‹¤','ê°™ìŠµë‹ˆë‹¤ë§Œ','ê°™ì•˜ì–´ìš”','ê°™ì•˜ë„¤ìš”',\n",
        "    'ë‹¤ë¥¸','ê±°ì˜','ê·¸ëŸ°ì§€','ì¶”ì²œ','ë¯¸ë¦¬','íŠ¹íˆ','ìˆì–´','í•˜ëŠ”','í•˜ê³ ','í–ˆë‹¤',\n",
        "\n",
        "    # ë¶ˆí•„ìš” ë°˜ë³µí˜• í‘œí˜„\n",
        "    'ë§ì•„ìš”','ë§ì•„ì„œ','ë§ê³ ','ë§ì€','ë§ë‹¤','ë§ìŒ','ë§ì•„','ë§ì•˜ì–´ìš”','ë§ì•˜ë„¤ìš”','ë§ì•˜ë˜',\n",
        "    'ìˆë‹¤','ìˆë„¤ìš”','ìˆìŠµë‹ˆë‹¤','ìˆì–´ìš”','ìˆì–´ì„œ','ìˆê³ ',\n",
        "\n",
        "    # ì¼ë°˜ ëª…ì‚¬ì„± ë¶ˆìš©ì–´\n",
        "    'ê³³','ê³³ì´','ê³³ë„','ì´ìš©','ì´ìš©í–ˆì–´ìš”','ì´ìš©í–ˆìŠµë‹ˆë‹¤','ì‹œê°„','ì‹œê°„ì´','ë•Œ','ë‚ ','ì •ë§','ì •ë„','ë¶€ë¶„','ê²½ìš°','ì´ë²ˆ',\n",
        "    'ì—ì„œ','ìœ¼ë¡œ','í•˜ì—¬','ë˜ì–´','ë˜ì–´ì„œ','ë˜ì–´ìˆì–´ìš”','ë˜ì–´ìˆìŠµë‹ˆë‹¤','ë˜ì–´ìˆë„¤ìš”',\n",
        "])\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 8ï¸âƒ£ ì „ì²´ ë¦¬ë·° ë‹¨ì–´ ë¹ˆë„ Top 50\n",
        "# ============================================\n",
        "all_text = \" \".join(df[text_col].astype(str))\n",
        "all_text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', ' ', all_text)\n",
        "filtered_words = [w for w in all_text.split() if w not in stopwords and len(w) > 1]\n",
        "word_freq = Counter(filtered_words)\n",
        "top50 = pd.DataFrame(word_freq.most_common(50), columns=['Word','Frequency'])\n",
        "print(\"\\n=== ğŸ·ï¸ ì „ì²´ ë¦¬ë·° Top 50 ë‹¨ì–´ ===\")\n",
        "print(top50)\n",
        "\n",
        "# ============================================\n",
        "# 9ï¸âƒ£ ë¶€ì • ë¦¬ë·°ë§Œ ì¶”ì¶œ â†’ í…Œë§ˆë³„ ë¶„ì„\n",
        "# ============================================\n",
        "neg_df = df[df['sentiment_label'] == 'negative']\n",
        "neg_text = \" \".join(neg_df[text_col].astype(str))\n",
        "neg_text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', ' ', neg_text)\n",
        "\n",
        "filtered_neg_words = [w for w in neg_text.split() if w not in stopwords and len(w) > 1]\n",
        "neg_freq = Counter(filtered_neg_words)\n",
        "top50_neg = pd.DataFrame(neg_freq.most_common(50), columns=['Word','Frequency'])\n",
        "print(\"\\n=== ğŸ”´ ë¶€ì • ë¦¬ë·° ë‚´ Top 50 ë‹¨ì–´ ===\")\n",
        "print(top50_neg)\n",
        "# ğŸ”´ ë¶€ì • ë¦¬ë·° ë‚´ ê°ì •ì–´ ì¤‘ì‹¬ ë‹¨ì–´ ì¶”ì¶œ\n",
        "negative_words = [\n",
        "    'ë³„ë¡œ','ë¶ˆì¹œì ˆ','ë¹„ì‹¸','ë”ì›Œ','ì¶¥','ì§€ë£¨','ì˜¤ë˜','ê¸°ë‹¤','í˜ë“¤','ë§‰íˆ','ëŠ¦','ë¶ˆí¸','í˜¼ì¡',\n",
        "    'ë”ëŸ½','ëƒ„ìƒˆ','ë³µì¡','ê³ ì¥','ì—†','ë¶€ì¡±','ì•„ì‰¬','ì‹¤ë§','ì§œì¦','ë¶ˆì¾Œ','ë‚¡','ì‹œë„ëŸ½','í•œì°¸'\n",
        "]\n",
        "\n",
        "neg_filtered = [w for w in filtered_neg_words if any(n in w for n in negative_words)]\n",
        "neg_freq_sentiment = Counter(neg_filtered)\n",
        "top50_neg_sentiment = pd.DataFrame(neg_freq_sentiment.most_common(50), columns=['Word','Frequency'])\n",
        "print(\"\\n=== ğŸ”´ ë¶€ì • ê°ì •ì–´ ì¤‘ì‹¬ Top 50 ===\")\n",
        "print(top50_neg_sentiment)\n",
        "\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# ğŸ”Ÿ í…Œë§ˆ í‚¤ì›Œë“œ ë¶„ë¥˜\n",
        "# ============================================\n",
        "theme_keywords = {\n",
        "    'Service': ['ì§ì›','ì¹œì ˆ','ë¶ˆì¹œì ˆ','ì‘ëŒ€','ì„œë¹„ìŠ¤','íƒœë„','ì„¤ëª…','ì§ì›ì´','ëŒ€ì‘'],\n",
        "    'Facility': ['í™”ì¥ì‹¤','ì²­ê²°','ê´€ë¦¬','ê¹¨ë—','ì •ë¹„','ì‹œì„¤','ìœ„ìƒ','ëƒ„ìƒˆ','ìƒíƒœ'],\n",
        "    'Price': ['ê°€ê²©','ë¹„ì‹¸','ì¿ í°','í• ì¸','ì…ì¥ë£Œ','ê°€ì„±ë¹„','ë¹„ìš©','ëˆ'],\n",
        "    'Waiting': ['ëŒ€ê¸°','ì¤„','ì›¨ì´íŒ…','ë¶ë¹”','ê¸°ë‹¤ë¦¼','í˜¼ì¡','ì§€ì—°','ì‹œê°„'],\n",
        "    'Attractions': ['ë†€ì´ê¸°êµ¬','í¼ë ˆì´ë“œ','ê³µì—°','ìŠ¤ë¦´','ì¬ë°Œ','ì²´í—˜','ì´ë²¤íŠ¸','íƒí—˜']\n",
        "}\n",
        "\n",
        "theme_counts = {}\n",
        "for theme, words in theme_keywords.items():\n",
        "    count = sum(neg_text.count(w) for w in words)\n",
        "    theme_counts[theme] = count\n",
        "\n",
        "theme_df = pd.DataFrame(theme_counts.items(), columns=['Theme','Frequency']).sort_values(by='Frequency', ascending=False)\n",
        "\n",
        "print(\"\\n=== ğŸ“Š ë¶€ì • ë¦¬ë·° í…Œë§ˆë³„ ë¹ˆë„ ===\")\n",
        "print(theme_df)\n",
        "\n",
        "# ì‹œê°í™”\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(theme_df['Theme'], theme_df['Frequency'], color='salmon')\n",
        "plt.title('Negative Review Themes')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Unm_5WVOxt97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# ğŸ§  KoELECTRA ë¦¬ë·° ê°ì„± ë¶„ì„ + í…Œë§ˆë³„ ë‹¨ì–´ë¶„ì„ ì™„ì „íŒ\n",
        "# ============================================\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ============================================\n",
        "# 1ï¸âƒ£ ë°ì´í„° ë¡œë“œ\n",
        "# ============================================\n",
        "df = pd.read_csv('/content/everland_reviews (1).csv')\n",
        "text_col = 'content'\n",
        "\n",
        "# ============================================\n",
        "# 2ï¸âƒ£ ëª¨ë¸ ë¡œë“œ (ì˜ë¬¸ ê°ì •ë¶„ì„ìš© ê³µê°œ ëª¨ë¸)\n",
        "# ============================================\n",
        "!pip install googletrans==4.0.0-rc1 > /dev/null\n",
        "\n",
        "from googletrans import Translator\n",
        "translator = Translator()\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from scipy.special import softmax\n",
        "\n",
        "MODEL_NAME = \"distilbert-base-uncased-finetuned-sst-2-english\"  # âœ… ê³µê°œ, ê°ì •ë¶„ì„ ì „ìš©\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# ============================================\n",
        "# 3ï¸âƒ£ ê°ì„± ì˜ˆì¸¡ í•¨ìˆ˜ (ë²ˆì—­ + ì˜ˆì¸¡)\n",
        "# ============================================\n",
        "def predict_sentiment(text):\n",
        "    if not isinstance(text, str) or text.strip() == \"\":\n",
        "        return 1  # ì¤‘ë¦½ ì²˜ë¦¬\n",
        "\n",
        "    try:\n",
        "        # âœ… í•œêµ­ì–´ â†’ ì˜ì–´ ë²ˆì—­\n",
        "        translated = translator.translate(text, src='ko', dest='en').text\n",
        "    except:\n",
        "        translated = text  # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ê·¸ëŒ€ë¡œ\n",
        "\n",
        "    inputs = tokenizer(translated, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    scores = softmax(outputs.logits.cpu().numpy()[0])\n",
        "    return 1 if scores[1] > scores[0] else 0  # 1=ê¸ì •, 0=ë¶€ì •\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 4ï¸âƒ£ ê°ì„± ë¶„ì„ ì‹¤í–‰\n",
        "# ============================================\n",
        "tqdm.pandas()\n",
        "df[text_col] = df[text_col].astype(str).fillna(\"\")\n",
        "df['sentiment'] = df[text_col].progress_apply(predict_sentiment)\n",
        "\n",
        "# 5ï¸âƒ£ ê°ì„± ë¼ë²¨ ë³€í™˜\n",
        "sentiment_map = {0: 'negative', 1: 'positive'}\n",
        "df['sentiment_label'] = df['sentiment'].map(sentiment_map)\n",
        "\n",
        "print(\"=== ğŸ¯ ê°ì„± ë¶„í¬ ===\")\n",
        "print(df['sentiment_label'].value_counts())\n",
        "\n",
        "# ============================================\n",
        "# 6ï¸âƒ£ ê°ì„± ë¶„í¬ ì‹œê°í™”\n",
        "# ============================================\n",
        "plt.figure(figsize=(5,4))\n",
        "df['sentiment_label'].value_counts().plot(kind='bar', color=['red','green'])\n",
        "plt.title('Everland Reviews Sentiment Distribution')\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# 7ï¸âƒ£ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ê°•í™” ë²„ì „)\n",
        "# ============================================\n",
        "# ============================================\n",
        "# ğŸ§¹ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ìµœì¢… ê°•í™” ë²„ì „)\n",
        "# ============================================\n",
        "stopwords = set([\n",
        "    # ì¡°ì‚¬Â·ì ‘ì†ì‚¬Â·ì¡°ë™ì‚¬\n",
        "    'ì€','ëŠ”','ì´','ê°€','ì„','ë¥¼','ì—','ì™€','ê³¼','ì˜','ë¡œ','ìœ¼ë¡œ','ì—ì„œ','ì—ê²Œ','ë³´ë‹¤','ìœ¼ë¡œì„œ',\n",
        "    'ë˜','ë˜í•œ','ê·¸ë¦¬ê³ ','í•˜ì§€ë§Œ','ê·¸ë˜ì„œ','ê·¸ëŸ¬ë‚˜','ë•Œë¬¸','ë•Œë¬¸ì—','ì´ëŸ°','ì €ëŸ°','ê·¸ëŸ°','ìš”','ë‹¤','ë“¯','ë“¯ì´',\n",
        "\n",
        "    # ê°íƒ„ì‚¬Â·ì¼ìƒí˜• ë¶€ì‚¬\n",
        "    'ë„ˆë¬´','ë§¤ìš°','ì •ë§','ì§„ì§œ','ì•„ì£¼','êµ‰ì¥íˆ','ì—„ì²­','ë§ì´','í•­ìƒ','ê°€ë”','ì–¸ì œë‚˜','ì •ë§ë¡œ',\n",
        "\n",
        "    # ê¸ì •Â·ê°•ì¡° í‘œí˜„ (ê°ì •ë¶„ì„ ì™œê³¡ ë°©ì§€ìš©)\n",
        "    'ì¢‹ì•„ìš”','ì¢‹ë„¤ìš”','ì¢‹ì•˜ì–´ìš”','ì¢‹ê³ ','ì¢‹ë‹¤','ì¢‹ì•˜ìŠµë‹ˆë‹¤','ì¢‹ìŠµë‹ˆë‹¤','ì¢‹ì€','ì¦ê±°ìš´','ì¦ê²ë‹¤',\n",
        "    'ìµœê³ ','ìµœê³ ì˜','ì—­ì‹œ','í™˜ìƒì˜','ì•„ë¦„ë‹µë‹¤','ë©‹ì ¸ìš”','ì˜ˆì˜ë‹¤','ì´ì˜ë‹¤','ì¬ë°Œë‹¤','ì¬ë¯¸ìˆë‹¤','ì¬ë°Œì–´ìš”',\n",
        "\n",
        "    # ì„œìˆ í˜• ì¢…ê²°ì–´Â·ì¡°ì‚¬\n",
        "    'ìˆì–´ìš”','ìˆë„¤ìš”','ìˆìŠµë‹ˆë‹¤','ìˆê³ ','ìˆì–´ì„œ','ìˆëŠ”','ìˆë‹¤','ë˜ì–´ìš”','ë©ë‹ˆë‹¤','ë˜ë„¤ìš”','ë¼ìš”',\n",
        "    'í•©ë‹ˆë‹¤','í–ˆì–´ìš”','í–ˆë„¤ìš”','í–ˆêµ¬ìš”','í–ˆìŠµë‹ˆë‹¤','í–ˆë‹µë‹ˆë‹¤','í•˜ë„¤ìš”','í•´ìš”','í•œë‹¤ê³ ','í•œë‹¤','í•˜ê³ ','í•˜ëŠ”','í–ˆì„',\n",
        "    'ì˜€ì–´ìš”','ì´ì—ˆì–´ìš”','ì—ìš”','ì´ì—ìš”','ë„¤ìš”','ì´ë„¤ìš”','ì˜€ìŠµë‹ˆë‹¤','í–ˆìŠµë‹ˆë‹¤','ì˜€ë„¤ìš”','ì´ì—ˆë„¤ìš”','í–ˆë„¤ìš”','í–ˆì–´ìš”',\n",
        "\n",
        "    # ê°íƒ„ì‚¬, ëŒ€í™”ì²´\n",
        "    'ã…‹ã…‹','ã…ã…','ã… ã… ','ã…œã…œ','í•˜í•˜','í—','ì™€','ìš°ì™€','ì•¼','ì•„','ìŒ','ì‘','íœ´','í•˜','ì—íœ´','ì–´íœ´','ì§±',\n",
        "\n",
        "    # ì¤‘ë¦½ ëª…ì‚¬Â·ë¶ˆí•„ìš” ë‹¨ì–´ (ì˜ë¯¸ ì—†ëŠ” ë¬¸ë§¥ì–´)\n",
        "    'ì‚¬ëŒ','ì‚¬ëŒì´','ì‚¬ëŒë“¤','ì‚¬ëŒë„','ì‚¬ëŒì€','ì‚¬ëŒë“¤ì´',\n",
        "    'ê·¸ë˜ë„','ìŠ¤ë§ˆíŠ¸','ê°”ëŠ”ë°','í•¨ê»˜','ë‹¤ë§Œ','ë‚˜ë¼','ê·¼ë°','íŠ¹íˆ','ê±°ì˜','ë‹¤ë¥¸','ê·¸ëŸ°ì§€','ì¶”ì²œ','ë¯¸ë¦¬',\n",
        "    'ì—†ëŠ”','ì—†ë„¤ìš”','ì—†ì–´ìš”','ì—†ê³ ','ì—†ìœ¼ë©°','ì—†ì—ˆì–´ìš”','ì—†ì§€ë§Œ','ì—†ë„¤ìš”','ì—†ìŒ','ì—†ì´','ì—†ìŠµë‹ˆë‹¤',\n",
        "    'ë†€ì´ê¸°êµ¬','ë†€ì´ê¸°êµ¬ë¥¼','ë†€ì´ê¸°êµ¬ê°€','ë†€ì´ê¸°êµ¬ëŠ”','ë†€ì´ê³µì›','ë†€ì´ë™ì‚°',\n",
        "    'íƒ€ê³ ','íƒ€ë‹¤','ë³´ê³ ','ë´¤ì–´ìš”','ë´¤ë‹¤','ë³´ë‹ˆ','ë³´ë‹ˆê¹Œ','ë´¤ìŠµë‹ˆë‹¤','ë´¤ë„¤ìš”',\n",
        "    'í‰ì¼','í‰ì¼ì—','ì£¼ë§','ì˜¤ëœë§Œì—','ì•„ì´ë“¤ì´','ì•„ì´ë“¤ê³¼','ì•„ì´','ì•„ì´ë“¤','ì•„ì´ë‘','ì•„ì´ì™€',\n",
        "    'ì‚¬íŒŒë¦¬','ë¡œìŠ¤íŠ¸ë°¸ë¦¬','ë¶ˆê½ƒë†€ì´','í• ë¡œìœˆ','í¼ë ˆì´ë“œ','ìŠ¤ë§ˆíŠ¸','ë‚˜ë¼','ëŒ€ê¸°','ì¤„ì„œê¸°',\n",
        "    'ê°™ì•„ìš”','ê°™ì´','ê°™ì€','ê°™ë„¤ìš”','ê°™ìŠµë‹ˆë‹¤','ê°™ìŠµë‹ˆë‹¤ë§Œ','ê°™ì•˜ì–´ìš”','ê°™ì•˜ë„¤ìš”',\n",
        "    'ë‹¤ë¥¸','ê±°ì˜','ê·¸ëŸ°ì§€','ì¶”ì²œ','ë¯¸ë¦¬','íŠ¹íˆ','ìˆì–´','í•˜ëŠ”','í•˜ê³ ','í–ˆë‹¤',\n",
        "\n",
        "    # ë¶ˆí•„ìš” ë°˜ë³µí˜• í‘œí˜„\n",
        "    'ë§ì•„ìš”','ë§ì•„ì„œ','ë§ê³ ','ë§ì€','ë§ë‹¤','ë§ìŒ','ë§ì•„','ë§ì•˜ì–´ìš”','ë§ì•˜ë„¤ìš”','ë§ì•˜ë˜',\n",
        "    'ìˆë‹¤','ìˆë„¤ìš”','ìˆìŠµë‹ˆë‹¤','ìˆì–´ìš”','ìˆì–´ì„œ','ìˆê³ ',\n",
        "\n",
        "    # ì¼ë°˜ ëª…ì‚¬ì„± ë¶ˆìš©ì–´\n",
        "    'ê³³','ê³³ì´','ê³³ë„','ì´ìš©','ì´ìš©í–ˆì–´ìš”','ì´ìš©í–ˆìŠµë‹ˆë‹¤','ì‹œê°„','ì‹œê°„ì´','ë•Œ','ë‚ ','ì •ë§','ì •ë„','ë¶€ë¶„','ê²½ìš°','ì´ë²ˆ',\n",
        "    'ì—ì„œ','ìœ¼ë¡œ','í•˜ì—¬','ë˜ì–´','ë˜ì–´ì„œ','ë˜ì–´ìˆì–´ìš”','ë˜ì–´ìˆìŠµë‹ˆë‹¤','ë˜ì–´ìˆë„¤ìš”',\n",
        "\n",
        "    'ê°€ì„œ','ê°€ë©´','ì—†ì–´ì„œ','ì—†ê³ ','ì—†ë„¤ìš”','ì—†ì–´ìš”','ì—†ì´','ì—†ì—ˆì–´ìš”','ì—†ìŠµë‹ˆë‹¤','ì—†ì§€ë§Œ',\n",
        " 'ì¡°ê¸ˆ','ê·¸ëƒ¥','ë°”ë¡œ','í•´ì„œ','í•˜ê³ ','í•˜ë©°','í–ˆë‹¤','í•œë‹¤','í•˜ëŠ”','ìˆê³ ','ìˆì–´ìš”','ìˆë„¤ìš”',\n",
        " 'ë‹¤ì‹œ','ì¼ì°','ì™€ì„œ','í•´ì„œ','í•˜ë©´','í•˜ë ¤ê³ ','ê°™ì•„ìš”','ê°™ì´','ê°™ì€','ê°™ë„¤ìš”','ê°™ì•˜ìŠµë‹ˆë‹¤',\n",
        " 'ì œì¼','ëª¨ë‘','ê°€ì¥','ê·¸ëŸ°ë°','ì´ì œ','ë¹„í•´','ì•Šê³ ','ì•Šì€','ì•Šì•„ìš”','ì•Šì•˜ìŠµë‹ˆë‹¤','ì•Šë„¤ìš”',\n",
        " 'ì¤„ì´','ì¤„ì„œì„œ','ì¤„ì„œê¸°','ê¸°ë‹¤ë¦¬ëŠ”','ê¸°ë‹¤ë ¤ì•¼','ê¸°ë‹¤ë¦¼','ê¸°ë‹¤ë¦¬ê³ ','ê¸°ë‹¤ë ¤','ê¸°ë‹¤ë¦¬ëŠ”ë°','ê¸°ë‹¤ë¦°',\n",
        " 'ë†€ì´ê¸°êµ¬','ë†€ì´ê¸°êµ¬ë„','ë†€ì´ê¸°êµ¬ëŠ”','ë†€ì´ê³µì›','ë†€ì´','í¼ë ˆì´ë“œ','ì‚¬íŒŒë¦¬','ë¡œìŠ¤íŠ¸ë°¸ë¦¬',\n",
        " 'ìŠ¤ë§ˆíŠ¸ì¤„ì„œê¸°','ìµìŠ¤í”„ë ˆìŠ¤','í‹°ìµìŠ¤í”„ë ˆìŠ¤','ì•„ë§ˆì¡´','í‘¸ë°”ì˜¤','ë¶ˆê½ƒë†€ì´',\n",
        " 'ì‹œê°„','ë‚ ì”¨','ì˜¤í›„','ëª¨ë“ ','ë§ì§€','ì¡°ê¸ˆ','ìƒê°ë³´ë‹¤','ë¹„ê°€','ë¹„í•´',\n",
        " 'ì¢‹ìŒ','ì¢‹ì•„ìš”','ì¢‹ë„¤ìš”','ì¢‹ì•˜ìŠµë‹ˆë‹¤','ì¢‹ì•˜ìŠµë‹ˆë‹¤','ì¢‹ì•˜ì–´ìš”','ì¢‹ì€ë°',\n",
        " 'ë‹¤ë§Œ','ê·¼ë°','ê·¸ë˜ë„','íŠ¹íˆ','ê±°ì˜','ë˜','ë˜í•œ','ê·¸ë¦¬ê³ ','í•˜ì§€ë§Œ','ê·¸ëŸ¬ë‚˜',\n",
        " 'ì—ì„œ','ìœ¼ë¡œ','ìœ¼ë¡œì„œ','ì´ë‚˜','ê±°ë‚˜','ë³´ë‹¤','ì²˜ëŸ¼','ìœ¼ë¡œì¨','ë“±','ê°™ì€','ë•Œë¬¸','ë•Œë¬¸ì—',\n",
        " 'ìš°ë¦¬','ì €í¬','ì €','ë‚´','ë³¸ì¸','ìì‹ ','ë„ˆë¬´','ë§¤ìš°','ì •ë§','ì§„ì§œ','ì•„ì£¼','êµ‰ì¥íˆ','ì—„ì²­',\n",
        " 'ë§ì•„ìš”','ë§ê³ ','ë§ì•„ì„œ','ë§ì€','ë§ë‹¤','ë§ì•„','ë§ìŒ','ë§ì•˜ì–´ìš”',\n",
        " 'í•˜ëŠ”','í–ˆë‹¤','í–ˆê³ ','í•´ì„œ','í•˜ë ¤ê³ ','í•˜ë©°','í•˜ë ¤ë‹ˆ','í•˜ê³ ','í•˜ë‹ˆ','í•œë‹¤',\n",
        " 'ì—†ëŠ”','ì—†ì§€ë§Œ','ì—†ì—ˆì§€ë§Œ','ì—†ì–´ì„œ','ì—†ë‹¤','ì—†ì—ˆë‹¤',\n",
        " 'ìˆë‹¤','ìˆì—ˆì–´ìš”','ìˆì—ˆë„¤ìš”','ìˆì—ˆë˜','ìˆì–´ì„œ','ìˆìŠµë‹ˆë‹¤','ìˆë„¤ìš”','ìˆì–´ìš”', 'ì—ë²„ëœë“œ'\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 8ï¸âƒ£ ì „ì²´ ë¦¬ë·° ë‹¨ì–´ ë¹ˆë„ Top 50\n",
        "# ============================================\n",
        "all_text = \" \".join(df[text_col].astype(str))\n",
        "all_text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', ' ', all_text)\n",
        "filtered_words = [w for w in all_text.split() if w not in stopwords and len(w) > 1]\n",
        "word_freq = Counter(filtered_words)\n",
        "top50 = pd.DataFrame(word_freq.most_common(50), columns=['Word','Frequency'])\n",
        "print(\"\\n=== ğŸ·ï¸ ì „ì²´ ë¦¬ë·° Top 50 ë‹¨ì–´ ===\")\n",
        "print(top50)\n",
        "\n",
        "# ============================================\n",
        "# 9ï¸âƒ£ ë¶€ì • ë¦¬ë·°ë§Œ ì¶”ì¶œ â†’ í…Œë§ˆë³„ ë¶„ì„\n",
        "# ============================================\n",
        "neg_df = df[df['sentiment_label'] == 'negative']\n",
        "neg_text = \" \".join(neg_df[text_col].astype(str))\n",
        "neg_text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', ' ', neg_text)\n",
        "\n",
        "filtered_neg_words = [w for w in neg_text.split() if w not in stopwords and len(w) > 1]\n",
        "neg_freq = Counter(filtered_neg_words)\n",
        "top50_neg = pd.DataFrame(neg_freq.most_common(50), columns=['Word','Frequency'])\n",
        "print(\"\\n=== ğŸ”´ ë¶€ì • ë¦¬ë·° ë‚´ Top 50 ë‹¨ì–´ ===\")\n",
        "print(top50_neg)\n",
        "# ğŸ”´ ë¶€ì • ë¦¬ë·° ë‚´ ê°ì •ì–´ ì¤‘ì‹¬ ë‹¨ì–´ ì¶”ì¶œ\n",
        "negative_words = [\n",
        "    'ë³„ë¡œ','ë¶ˆì¹œì ˆ','ë¹„ì‹¸','ë”ì›Œ','ì¶¥','ì§€ë£¨','ì˜¤ë˜','ê¸°ë‹¤','í˜ë“¤','ë§‰íˆ','ëŠ¦','ë¶ˆí¸','í˜¼ì¡',\n",
        "    'ë”ëŸ½','ëƒ„ìƒˆ','ë³µì¡','ê³ ì¥','ì—†','ë¶€ì¡±','ì•„ì‰¬','ì‹¤ë§','ì§œì¦','ë¶ˆì¾Œ','ë‚¡','ì‹œë„ëŸ½','í•œì°¸'\n",
        "]\n",
        "\n",
        "neg_filtered = [w for w in filtered_neg_words if any(n in w for n in negative_words)]\n",
        "neg_freq_sentiment = Counter(neg_filtered)\n",
        "top50_neg_sentiment = pd.DataFrame(neg_freq_sentiment.most_common(50), columns=['Word','Frequency'])\n",
        "print(\"\\n=== ğŸ”´ ë¶€ì • ê°ì •ì–´ ì¤‘ì‹¬ Top 50 ===\")\n",
        "print(top50_neg_sentiment)\n",
        "\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# ğŸ”Ÿ í…Œë§ˆ í‚¤ì›Œë“œ ë¶„ë¥˜\n",
        "# ============================================\n",
        "theme_keywords = {\n",
        "    'Service': ['ì§ì›','ì¹œì ˆ','ë¶ˆì¹œì ˆ','ì‘ëŒ€','ì„œë¹„ìŠ¤','íƒœë„','ì„¤ëª…','ì§ì›ì´','ëŒ€ì‘'],\n",
        "    'Facility': ['í™”ì¥ì‹¤','ì²­ê²°','ê´€ë¦¬','ê¹¨ë—','ì •ë¹„','ì‹œì„¤','ìœ„ìƒ','ëƒ„ìƒˆ','ìƒíƒœ'],\n",
        "    'Price': ['ê°€ê²©','ë¹„ì‹¸','ì¿ í°','í• ì¸','ì…ì¥ë£Œ','ê°€ì„±ë¹„','ë¹„ìš©','ëˆ'],\n",
        "    'Waiting': ['ëŒ€ê¸°','ì¤„','ì›¨ì´íŒ…','ë¶ë¹”','ê¸°ë‹¤ë¦¼','í˜¼ì¡','ì§€ì—°','ì‹œê°„'],\n",
        "    'Attractions': ['ë†€ì´ê¸°êµ¬','í¼ë ˆì´ë“œ','ê³µì—°','ìŠ¤ë¦´','ì¬ë°Œ','ì²´í—˜','ì´ë²¤íŠ¸','íƒí—˜']\n",
        "}\n",
        "\n",
        "theme_counts = {}\n",
        "for theme, words in theme_keywords.items():\n",
        "    count = sum(neg_text.count(w) for w in words)\n",
        "    theme_counts[theme] = count\n",
        "\n",
        "theme_df = pd.DataFrame(theme_counts.items(), columns=['Theme','Frequency']).sort_values(by='Frequency', ascending=False)\n",
        "\n",
        "print(\"\\n=== ğŸ“Š ë¶€ì • ë¦¬ë·° í…Œë§ˆë³„ ë¹ˆë„ ===\")\n",
        "print(theme_df)\n",
        "\n",
        "# ì‹œê°í™”\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(theme_df['Theme'], theme_df['Frequency'], color='salmon')\n",
        "plt.title('Negative Review Themes')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bIVMScU3XUV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# ğŸ§  KoELECTRA ë¦¬ë·° ê°ì„± ë¶„ì„ + í…Œë§ˆë³„ ë‹¨ì–´ë¶„ì„ ì™„ì „íŒ\n",
        "# ============================================\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ============================================\n",
        "# 1ï¸âƒ£ ë°ì´í„° ë¡œë“œ\n",
        "# ============================================\n",
        "df = pd.read_csv('/content/everland_reviews (1).csv')\n",
        "text_col = 'content'\n",
        "\n",
        "# ============================================\n",
        "# 2ï¸âƒ£ ëª¨ë¸ ë¡œë“œ (ì˜ë¬¸ ê°ì •ë¶„ì„ìš© ê³µê°œ ëª¨ë¸)\n",
        "# ============================================\n",
        "!pip install googletrans==4.0.0-rc1 > /dev/null\n",
        "\n",
        "from googletrans import Translator\n",
        "translator = Translator()\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from scipy.special import softmax\n",
        "\n",
        "MODEL_NAME = \"distilbert-base-uncased-finetuned-sst-2-english\"  # âœ… ê³µê°œ, ê°ì •ë¶„ì„ ì „ìš©\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# ============================================\n",
        "# 3ï¸âƒ£ ê°ì„± ì˜ˆì¸¡ í•¨ìˆ˜ (ë²ˆì—­ + ì˜ˆì¸¡)\n",
        "# ============================================\n",
        "def predict_sentiment(text):\n",
        "    if not isinstance(text, str) or text.strip() == \"\":\n",
        "        return 1  # ì¤‘ë¦½ ì²˜ë¦¬\n",
        "\n",
        "    try:\n",
        "        # âœ… í•œêµ­ì–´ â†’ ì˜ì–´ ë²ˆì—­\n",
        "        translated = translator.translate(text, src='ko', dest='en').text\n",
        "    except:\n",
        "        translated = text  # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ê·¸ëŒ€ë¡œ\n",
        "\n",
        "    inputs = tokenizer(translated, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    scores = softmax(outputs.logits.cpu().numpy()[0])\n",
        "    return 1 if scores[1] > scores[0] else 0  # 1=ê¸ì •, 0=ë¶€ì •\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 4ï¸âƒ£ ê°ì„± ë¶„ì„ ì‹¤í–‰\n",
        "# ============================================\n",
        "tqdm.pandas()\n",
        "df[text_col] = df[text_col].astype(str).fillna(\"\")\n",
        "df['sentiment'] = df[text_col].progress_apply(predict_sentiment)\n",
        "\n",
        "# 5ï¸âƒ£ ê°ì„± ë¼ë²¨ ë³€í™˜\n",
        "sentiment_map = {0: 'negative', 1: 'positive'}\n",
        "df['sentiment_label'] = df['sentiment'].map(sentiment_map)\n",
        "\n",
        "print(\"=== ğŸ¯ ê°ì„± ë¶„í¬ ===\")\n",
        "print(df['sentiment_label'].value_counts())\n",
        "\n",
        "# ============================================\n",
        "# 6ï¸âƒ£ ê°ì„± ë¶„í¬ ì‹œê°í™”\n",
        "# ============================================\n",
        "plt.figure(figsize=(5,4))\n",
        "df['sentiment_label'].value_counts().plot(kind='bar', color=['red','green'])\n",
        "plt.title('Everland Reviews Sentiment Distribution')\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# 7ï¸âƒ£ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ê°•í™” ë²„ì „)\n",
        "# ============================================\n",
        "# ============================================\n",
        "# ğŸ§¹ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (ìµœì¢… ê°•í™” ë²„ì „)\n",
        "# ============================================\n",
        "stopwords = set([\n",
        "    # ì¡°ì‚¬Â·ì ‘ì†ì‚¬Â·ì¡°ë™ì‚¬\n",
        "    'ì€','ëŠ”','ì´','ê°€','ì„','ë¥¼','ì—','ì™€','ê³¼','ì˜','ë¡œ','ìœ¼ë¡œ','ì—ì„œ','ì—ê²Œ','ë³´ë‹¤','ìœ¼ë¡œì„œ',\n",
        "    'ë˜','ë˜í•œ','ê·¸ë¦¬ê³ ','í•˜ì§€ë§Œ','ê·¸ë˜ì„œ','ê·¸ëŸ¬ë‚˜','ë•Œë¬¸','ë•Œë¬¸ì—','ì´ëŸ°','ì €ëŸ°','ê·¸ëŸ°','ìš”','ë‹¤','ë“¯','ë“¯ì´',\n",
        "\n",
        "  'ê²ƒ', 'ìˆ˜', 'ë“¤', 'ê·¸', 'ì´', 'ì €', 'ë°', 'ë”', 'ë“±', 'ê±°', 'ì œ', 'ì¢€',\n",
        "    'ì—ì„œ', 'í•˜ë‹¤', 'ì…ë‹ˆë‹¤', 'ìœ¼ë¡œ', 'í•˜ëŠ”', 'ì—ë„', 'í•˜ê²Œ', 'í•˜ê³ ', 'í•˜ê³ ì‹¶ë‹¤',\n",
        "    'ë„ˆë¬´', 'ë§¤ìš°', 'ì •ë§', 'ì§„ì§œ', 'ë•Œë¬¸', 'ìœ„í•´', 'í•˜ì§€ë§Œ', 'ê±°ë‚˜', 'ë³´ë‹¤', 'í•´ì„œ',\n",
        "    'ì—ì„œëŠ”', 'ìœ¼ë¡œì„œ', 'ìœ¼ë¡œì¨', 'í•˜ë©´ì„œ', 'ì´ë©´', 'ì˜€ë‹¤', 'ì˜€ë‹¤ê°€', 'ì´ë‚˜', 'ì—ëŠ”', 'ê³³', 'ë¶„', 'ë•Œ', 'íƒˆ', 'ì„œê¸°', 'ë…„', 'ê°œ', 'ì‹œ', 'ë¹„', 'í•˜ë‚˜', 'ë‚ ',\n",
        "    'ê·¸ëƒ¥', 'ì •ë„', 'ì•ˆ', 'ëŠë‚Œ', 'ê±°ì˜', 'ì´ìƒ', 'íƒ€ê³ ', 'í•œë²ˆ', 'ë§', 'ë³´ê³ ', 'ë§Œ', 'ë“¯', 'ë³„ë¡œ', 'ë„', 'ë­', 'ì›', 'ì…˜', 'í•¨', 'ìŠ¤', 'ë°¸ë¦¬', 'ì „', 'ë‹¤ì‹œ', 'ì–´',\n",
        "    'ì¼', 'í”„ë ˆ', 'ê°€ì§€', 'ì™œ', 'í•´', 'ëª»', 'ì• ', 'ê¸°ë¶„', 'ë³¼', 'ë²ˆ', 'ì´ì œ', 'ì—­ì‹œ', 'ê°€ë©´', 'ë§Œì›', 'ì„', 'ë¬¸', 'ììœ ', 'ì¥ì†Œ', 'ì§€ê¸ˆ', 'í•œêµ­',\n",
        "    'í• ì¸', 'ë‚´', 'ëª‡ê°œ', 'ì¡°ê¸ˆ', 'ì›”','ê¼­', 'ë°”ë¡œ', 'ì œëŒ€ë¡œ', 'ê¸°ë„', 'ë ˆ', 'ì°¬ìŠ¤', 'ê²Œ', 'ê°€ì„¸', 'í›„', 'ì•', 'ìš”', 'ì ', 'ë‹¤ìŒ', 'ë˜', 'ì´í›„',\n",
        "    'ì˜¨', 'ë­”ê°€', 'ê³³ë„', 'ì ì ', 'ì¤‘', 'ë™ì•ˆ', 'ëŒ€í•œ', 'ê³„ì†', 'ìì£¼', 'ë˜í•œ', 'ìŒ', 'ì´ë²ˆ', 'í‹°', 'ì™„ì „','ëª¨ë“ ', 'ìš”ì¦˜', 'ì „ì²´', 'ë‚˜ë¼', 'ë…„ì „',\n",
        "    'ë² ì´', 'í•­ìƒ', 'ë•', 'ì™¸', 'ë¬´ìŠ¨', 'ì¸ì§€', 'ì•ˆí•´', 'ì´ˆ', 'ìª½', 'í† í”¼ì•„', 'ê°€ê¸°', 'ìš°ë¦¬', 'ì œë¡œ', 'ëŠ˜', 'ë°ë¦¬',\n",
        "    'ì´ë‹¤', 'ì˜¤ë‹¤', 'ì—†ë‹¤', 'ê³¼', 'ê°™ë‹¤', 'ë†€ë‹¤', 'ë‹¤', 'ìë‹¤', 'í•œ', 'ì™€', 'ê³ ', 'ê¹Œì§€', 'ìˆë‹¤', 'ì—†ë‹¤', 'ì˜¤ë‹¤', 'í•œ', 'ì´ë‹¤', 'ë‹¤', 'ê°™ë‹¤',\n",
        "    'ê³ ', 'ê³¼', 'ì•„ë‹ˆë‹¤', 'ì„œ', 'ê·¸ë¦¬ê³ ', 'ì´ë¼', 'ë‚˜', 'ì¸ë°', 'ì—', 'ê°€', 'ì„', 'ëŠ”', 'ì€', 'ê°€ë‹¤', 'ì˜', 'ë¥¼', 'ë¡œ', 'ë˜ë‹¤', 'ì•Šë‹¤', 'ëŒ€', 'íƒ€ë‹¤',\n",
        "    'ì ', 'ê·¸ë˜ë„', 'ë˜ì–´ë‹¤', 'ë„˜ë‹¤', 'ë§ì´', 'ìƒê°', 'ê¸°ë³¸', 'ë¨¹ë‹¤', 'ê¸°', 'ê·¸ë ‡ë‹¤', 'ë“¤ì–´ê°€ë‹¤', 'ì„œë‹¤', 'ê·¸ë˜ë„' ,\n",
        "    'ë¶€í„°', 'ì˜ˆì „', 'ë°›ë‹¤', 'ì‹¶ë‹¤', 'ì¸', 'ë§Œì—', 'ì ˆëŒ€', 'ìµìŠ¤',\n",
        "    'ì•ˆë˜ë‹¤', 'ë‘', 'ë¹„ë‹¤', 'ë‹¤ë…€ì˜¤ë‹¤', 'ì—”', 'ê°™ë‹¤', 'í•˜', 'ë‚´ë‹¤', 'ë‹ˆ', 'ë‚˜ì˜¤ë‹¤', 'ë“¤ë‹¤'\n",
        "\n",
        "    # ê°íƒ„ì‚¬Â·ì¼ìƒí˜• ë¶€ì‚¬\n",
        "    'ë„ˆë¬´','ë§¤ìš°','ì •ë§','ì§„ì§œ','ì•„ì£¼','êµ‰ì¥íˆ','ì—„ì²­','ë§ì´','í•­ìƒ','ê°€ë”','ì–¸ì œë‚˜','ì •ë§ë¡œ',\n",
        "\n",
        "    # ê¸ì •Â·ê°•ì¡° í‘œí˜„ (ê°ì •ë¶„ì„ ì™œê³¡ ë°©ì§€ìš©)\n",
        "    'ì¢‹ì•„ìš”','ì¢‹ë„¤ìš”','ì¢‹ì•˜ì–´ìš”','ì¢‹ê³ ','ì¢‹ë‹¤','ì¢‹ì•˜ìŠµë‹ˆë‹¤','ì¢‹ìŠµë‹ˆë‹¤','ì¢‹ì€','ì¦ê±°ìš´','ì¦ê²ë‹¤',\n",
        "    'ìµœê³ ','ìµœê³ ì˜','ì—­ì‹œ','í™˜ìƒì˜','ì•„ë¦„ë‹µë‹¤','ë©‹ì ¸ìš”','ì˜ˆì˜ë‹¤','ì´ì˜ë‹¤','ì¬ë°Œë‹¤','ì¬ë¯¸ìˆë‹¤','ì¬ë°Œì–´ìš”',\n",
        "\n",
        "    # ì„œìˆ í˜• ì¢…ê²°ì–´Â·ì¡°ì‚¬\n",
        "    'ìˆì–´ìš”','ìˆë„¤ìš”','ìˆìŠµë‹ˆë‹¤','ìˆê³ ','ìˆì–´ì„œ','ìˆëŠ”','ìˆë‹¤','ë˜ì–´ìš”','ë©ë‹ˆë‹¤','ë˜ë„¤ìš”','ë¼ìš”',\n",
        "    'í•©ë‹ˆë‹¤','í–ˆì–´ìš”','í–ˆë„¤ìš”','í–ˆêµ¬ìš”','í–ˆìŠµë‹ˆë‹¤','í–ˆë‹µë‹ˆë‹¤','í•˜ë„¤ìš”','í•´ìš”','í•œë‹¤ê³ ','í•œë‹¤','í•˜ê³ ','í•˜ëŠ”','í–ˆì„',\n",
        "    'ì˜€ì–´ìš”','ì´ì—ˆì–´ìš”','ì—ìš”','ì´ì—ìš”','ë„¤ìš”','ì´ë„¤ìš”','ì˜€ìŠµë‹ˆë‹¤','í–ˆìŠµë‹ˆë‹¤','ì˜€ë„¤ìš”','ì´ì—ˆë„¤ìš”','í–ˆë„¤ìš”','í–ˆì–´ìš”',\n",
        "\n",
        "    # ê°íƒ„ì‚¬, ëŒ€í™”ì²´\n",
        "    'ã…‹ã…‹','ã…ã…','ã… ã… ','ã…œã…œ','í•˜í•˜','í—','ì™€','ìš°ì™€','ì•¼','ì•„','ìŒ','ì‘','íœ´','í•˜','ì—íœ´','ì–´íœ´','ì§±',\n",
        "\n",
        "    # ì¤‘ë¦½ ëª…ì‚¬Â·ë¶ˆí•„ìš” ë‹¨ì–´ (ì˜ë¯¸ ì—†ëŠ” ë¬¸ë§¥ì–´)\n",
        "    'ì‚¬ëŒ','ì‚¬ëŒì´','ì‚¬ëŒë“¤','ì‚¬ëŒë„','ì‚¬ëŒì€','ì‚¬ëŒë“¤ì´',\n",
        "    'ê·¸ë˜ë„','ìŠ¤ë§ˆíŠ¸','ê°”ëŠ”ë°','í•¨ê»˜','ë‹¤ë§Œ','ë‚˜ë¼','ê·¼ë°','íŠ¹íˆ','ê±°ì˜','ë‹¤ë¥¸','ê·¸ëŸ°ì§€','ì¶”ì²œ','ë¯¸ë¦¬',\n",
        "    'ì—†ëŠ”','ì—†ë„¤ìš”','ì—†ì–´ìš”','ì—†ê³ ','ì—†ìœ¼ë©°','ì—†ì—ˆì–´ìš”','ì—†ì§€ë§Œ','ì—†ë„¤ìš”','ì—†ìŒ','ì—†ì´','ì—†ìŠµë‹ˆë‹¤',\n",
        "    'ë†€ì´ê¸°êµ¬','ë†€ì´ê¸°êµ¬ë¥¼','ë†€ì´ê¸°êµ¬ê°€','ë†€ì´ê¸°êµ¬ëŠ”','ë†€ì´ê³µì›','ë†€ì´ë™ì‚°',\n",
        "    'íƒ€ê³ ','íƒ€ë‹¤','ë³´ê³ ','ë´¤ì–´ìš”','ë´¤ë‹¤','ë³´ë‹ˆ','ë³´ë‹ˆê¹Œ','ë´¤ìŠµë‹ˆë‹¤','ë´¤ë„¤ìš”',\n",
        "    'í‰ì¼','í‰ì¼ì—','ì£¼ë§','ì˜¤ëœë§Œì—','ì•„ì´ë“¤ì´','ì•„ì´ë“¤ê³¼','ì•„ì´','ì•„ì´ë“¤','ì•„ì´ë‘','ì•„ì´ì™€',\n",
        "    'ì‚¬íŒŒë¦¬','ë¡œìŠ¤íŠ¸ë°¸ë¦¬','ë¶ˆê½ƒë†€ì´','í• ë¡œìœˆ','í¼ë ˆì´ë“œ','ìŠ¤ë§ˆíŠ¸','ë‚˜ë¼','ëŒ€ê¸°','ì¤„ì„œê¸°',\n",
        "    'ê°™ì•„ìš”','ê°™ì´','ê°™ì€','ê°™ë„¤ìš”','ê°™ìŠµë‹ˆë‹¤','ê°™ìŠµë‹ˆë‹¤ë§Œ','ê°™ì•˜ì–´ìš”','ê°™ì•˜ë„¤ìš”',\n",
        "    'ë‹¤ë¥¸','ê±°ì˜','ê·¸ëŸ°ì§€','ì¶”ì²œ','ë¯¸ë¦¬','íŠ¹íˆ','ìˆì–´','í•˜ëŠ”','í•˜ê³ ','í–ˆë‹¤',\n",
        "\n",
        "    # ë¶ˆí•„ìš” ë°˜ë³µí˜• í‘œí˜„\n",
        "    'ë§ì•„ìš”','ë§ì•„ì„œ','ë§ê³ ','ë§ì€','ë§ë‹¤','ë§ìŒ','ë§ì•„','ë§ì•˜ì–´ìš”','ë§ì•˜ë„¤ìš”','ë§ì•˜ë˜',\n",
        "    'ìˆë‹¤','ìˆë„¤ìš”','ìˆìŠµë‹ˆë‹¤','ìˆì–´ìš”','ìˆì–´ì„œ','ìˆê³ ',\n",
        "\n",
        "    # ì¼ë°˜ ëª…ì‚¬ì„± ë¶ˆìš©ì–´\n",
        "    'ê³³','ê³³ì´','ê³³ë„','ì´ìš©','ì´ìš©í–ˆì–´ìš”','ì´ìš©í–ˆìŠµë‹ˆë‹¤','ì‹œê°„','ì‹œê°„ì´','ë•Œ','ë‚ ','ì •ë§','ì •ë„','ë¶€ë¶„','ê²½ìš°','ì´ë²ˆ',\n",
        "    'ì—ì„œ','ìœ¼ë¡œ','í•˜ì—¬','ë˜ì–´','ë˜ì–´ì„œ','ë˜ì–´ìˆì–´ìš”','ë˜ì–´ìˆìŠµë‹ˆë‹¤','ë˜ì–´ìˆë„¤ìš”',\n",
        "\n",
        "    'ê°€ì„œ','ê°€ë©´','ì—†ì–´ì„œ','ì—†ê³ ','ì—†ë„¤ìš”','ì—†ì–´ìš”','ì—†ì´','ì—†ì—ˆì–´ìš”','ì—†ìŠµë‹ˆë‹¤','ì—†ì§€ë§Œ',\n",
        " 'ì¡°ê¸ˆ','ê·¸ëƒ¥','ë°”ë¡œ','í•´ì„œ','í•˜ê³ ','í•˜ë©°','í–ˆë‹¤','í•œë‹¤','í•˜ëŠ”','ìˆê³ ','ìˆì–´ìš”','ìˆë„¤ìš”',\n",
        " 'ë‹¤ì‹œ','ì¼ì°','ì™€ì„œ','í•´ì„œ','í•˜ë©´','í•˜ë ¤ê³ ','ê°™ì•„ìš”','ê°™ì´','ê°™ì€','ê°™ë„¤ìš”','ê°™ì•˜ìŠµë‹ˆë‹¤',\n",
        " 'ì œì¼','ëª¨ë‘','ê°€ì¥','ê·¸ëŸ°ë°','ì´ì œ','ë¹„í•´','ì•Šê³ ','ì•Šì€','ì•Šì•„ìš”','ì•Šì•˜ìŠµë‹ˆë‹¤','ì•Šë„¤ìš”',\n",
        " 'ì¤„ì´','ì¤„ì„œì„œ','ì¤„ì„œê¸°','ê¸°ë‹¤ë¦¬ëŠ”','ê¸°ë‹¤ë ¤ì•¼','ê¸°ë‹¤ë¦¼','ê¸°ë‹¤ë¦¬ê³ ','ê¸°ë‹¤ë ¤','ê¸°ë‹¤ë¦¬ëŠ”ë°','ê¸°ë‹¤ë¦°',\n",
        " 'ë†€ì´ê¸°êµ¬','ë†€ì´ê¸°êµ¬ë„','ë†€ì´ê¸°êµ¬ëŠ”','ë†€ì´ê³µì›','ë†€ì´','í¼ë ˆì´ë“œ','ì‚¬íŒŒë¦¬','ë¡œìŠ¤íŠ¸ë°¸ë¦¬',\n",
        " 'ìŠ¤ë§ˆíŠ¸ì¤„ì„œê¸°','ìµìŠ¤í”„ë ˆìŠ¤','í‹°ìµìŠ¤í”„ë ˆìŠ¤','ì•„ë§ˆì¡´','í‘¸ë°”ì˜¤','ë¶ˆê½ƒë†€ì´',\n",
        " 'ì‹œê°„','ë‚ ì”¨','ì˜¤í›„','ëª¨ë“ ','ë§ì§€','ì¡°ê¸ˆ','ìƒê°ë³´ë‹¤','ë¹„ê°€','ë¹„í•´',\n",
        " 'ì¢‹ìŒ','ì¢‹ì•„ìš”','ì¢‹ë„¤ìš”','ì¢‹ì•˜ìŠµë‹ˆë‹¤','ì¢‹ì•˜ìŠµë‹ˆë‹¤','ì¢‹ì•˜ì–´ìš”','ì¢‹ì€ë°',\n",
        " 'ë‹¤ë§Œ','ê·¼ë°','ê·¸ë˜ë„','íŠ¹íˆ','ê±°ì˜','ë˜','ë˜í•œ','ê·¸ë¦¬ê³ ','í•˜ì§€ë§Œ','ê·¸ëŸ¬ë‚˜',\n",
        " 'ì—ì„œ','ìœ¼ë¡œ','ìœ¼ë¡œì„œ','ì´ë‚˜','ê±°ë‚˜','ë³´ë‹¤','ì²˜ëŸ¼','ìœ¼ë¡œì¨','ë“±','ê°™ì€','ë•Œë¬¸','ë•Œë¬¸ì—',\n",
        " 'ìš°ë¦¬','ì €í¬','ì €','ë‚´','ë³¸ì¸','ìì‹ ','ë„ˆë¬´','ë§¤ìš°','ì •ë§','ì§„ì§œ','ì•„ì£¼','êµ‰ì¥íˆ','ì—„ì²­',\n",
        " 'ë§ì•„ìš”','ë§ê³ ','ë§ì•„ì„œ','ë§ì€','ë§ë‹¤','ë§ì•„','ë§ìŒ','ë§ì•˜ì–´ìš”',\n",
        " 'í•˜ëŠ”','í–ˆë‹¤','í–ˆê³ ','í•´ì„œ','í•˜ë ¤ê³ ','í•˜ë©°','í•˜ë ¤ë‹ˆ','í•˜ê³ ','í•˜ë‹ˆ','í•œë‹¤',\n",
        " 'ì—†ëŠ”','ì—†ì§€ë§Œ','ì—†ì—ˆì§€ë§Œ','ì—†ì–´ì„œ','ì—†ë‹¤','ì—†ì—ˆë‹¤',\n",
        " 'ìˆë‹¤','ìˆì—ˆì–´ìš”','ìˆì—ˆë„¤ìš”','ìˆì—ˆë˜','ìˆì–´ì„œ','ìˆìŠµë‹ˆë‹¤','ìˆë„¤ìš”','ìˆì–´ìš”', 'ì—ë²„ëœë“œ'\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# 8ï¸âƒ£ ì „ì²´ ë¦¬ë·° ë‹¨ì–´ ë¹ˆë„ Top 50\n",
        "# ============================================\n",
        "all_text = \" \".join(df[text_col].astype(str))\n",
        "all_text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', ' ', all_text)\n",
        "filtered_words = [w for w in all_text.split() if w not in stopwords and len(w) > 1]\n",
        "word_freq = Counter(filtered_words)\n",
        "top50 = pd.DataFrame(word_freq.most_common(50), columns=['Word','Frequency'])\n",
        "print(\"\\n=== ğŸ·ï¸ ì „ì²´ ë¦¬ë·° Top 50 ë‹¨ì–´ ===\")\n",
        "print(top50)\n",
        "\n",
        "# ============================================\n",
        "# 9ï¸âƒ£ ë¶€ì • ë¦¬ë·°ë§Œ ì¶”ì¶œ â†’ í…Œë§ˆë³„ ë¶„ì„\n",
        "# ============================================\n",
        "neg_df = df[df['sentiment_label'] == 'negative']\n",
        "neg_text = \" \".join(neg_df[text_col].astype(str))\n",
        "neg_text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', ' ', neg_text)\n",
        "\n",
        "filtered_neg_words = [w for w in neg_text.split() if w not in stopwords and len(w) > 1]\n",
        "neg_freq = Counter(filtered_neg_words)\n",
        "top50_neg = pd.DataFrame(neg_freq.most_common(50), columns=['Word','Frequency'])\n",
        "print(\"\\n=== ğŸ”´ ë¶€ì • ë¦¬ë·° ë‚´ Top 50 ë‹¨ì–´ ===\")\n",
        "print(top50_neg)\n",
        "# ğŸ”´ ë¶€ì • ë¦¬ë·° ë‚´ ê°ì •ì–´ ì¤‘ì‹¬ ë‹¨ì–´ ì¶”ì¶œ\n",
        "negative_words = [\n",
        "    # ë¶ˆë§ŒÂ·ë¶€ì •ì  ê°ì •\n",
        "    'ë³„ë¡œ','ì‹«','ë¶ˆì¹œì ˆ','ë¶ˆì¾Œ','ì§œì¦','ì‹¤ë§','ì•„ì‰¬','ë¶ˆë§Œ','ë¶ˆí¸','ì§€ë£¨','ìµœì•…','ì—‰ë§','í”¼ê³¤','í˜ë“¤',\n",
        "    'ê³ ìƒ','ê·€ì°®','ë‹µë‹µ','ì§€ì³¤','ë¶ˆìŒ','ëŠ¦','ì§€ì—°','ë§‰íˆ','í˜¼ì¡','ë³µì¡','ë¶ë³','ë¶ë¹”',\n",
        "\n",
        "    # ì‹œì„¤Â·í™˜ê²½ ê´€ë ¨ ë¶€ì •\n",
        "    'ë”ëŸ½','ëƒ„ìƒˆ','ì§€ì €ë¶„','ë‚¡','ë§ê°€','ë¶€ì„œ','ê³ ì¥','ë°©ì¹˜','ê´€ë¦¬ì•ˆë¨','ì²­ì†Œì•ˆë¨','ìœ„ìƒì•ˆì¢‹','ë¯¸ë„ëŸ½',\n",
        "\n",
        "    # ê°€ê²©Â·ê²½ì œì  ë¶ˆë§Œ\n",
        "    'ë¹„ì‹¸','ë¹„ìŒˆ','ëˆì•„ê¹Œ','ê°€ê²©ë†’','ê°€ì„±ë¹„ì—†','ëˆê°’ëª»','í• ì¸ì—†','í˜œíƒì—†','ì…ì¥ë£Œë¹„ìŒˆ','ì¿ í°ì—†',\n",
        "\n",
        "    # ëŒ€ê¸°Â·ìš´ì˜ ë¬¸ì œ\n",
        "    'ê¸°ë‹¤','ê¸°ë‹¤ë ¤','ëŒ€ê¸°','ì¤„ì„œ','ì¤„ê¸¸','ì¤„ë§','ì¤„ì—„ì²­','ì¤„ì´ê¸¸','ì›¨ì´íŒ…','ëŒ€í˜¼ì¡','ì§€ì—°','ëŠ¦ìŒ',\n",
        "\n",
        "    # í™˜ê²½Â·ë‚ ì”¨Â·í”¼ë¡œ\n",
        "    'ë”ì›Œ','ì¶¥','ë¥','ì¶”ì›€','ë¹„ì™€ì„œ','ë¹„ì™€','í–‡ë¹›','í­ì—¼','í­ìš°','ë‚ ì”¨íƒ“','ëª¨ë˜ë°”ëŒ','ë¨¼ì§€',\n",
        "\n",
        "    # ê¸°íƒ€\n",
        "    'ë¶€ì¡±','ì—†','ì—†ì—ˆ','ì—†ë„¤ìš”','ì—†ì–´ìš”','ì—†ë‹¤','ì—†ìŒ','ì—†ì–´ì„œ','ì—†ì§€ë§Œ','ì—†ì—ˆìŒ',\n",
        "    'ë¬´ì„­','ê²','ìœ„í—˜','ë¶ˆì•ˆ','ì‹œë„ëŸ½','ì‹œë„ëŸ¬ì›€','í•œì°¸'\n",
        "]\n",
        "\n",
        "\n",
        "neg_filtered = [w for w in filtered_neg_words if any(n in w for n in negative_words)]\n",
        "neg_freq_sentiment = Counter(neg_filtered)\n",
        "top50_neg_sentiment = pd.DataFrame(neg_freq_sentiment.most_common(50), columns=['Word','Frequency'])\n",
        "print(\"\\n=== ğŸ”´ ë¶€ì • ê°ì •ì–´ ì¤‘ì‹¬ Top 50 ===\")\n",
        "print(top50_neg_sentiment)\n",
        "\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# ğŸ”Ÿ í…Œë§ˆ í‚¤ì›Œë“œ ë¶„ë¥˜\n",
        "# ============================================\n",
        "theme_keywords = {\n",
        "    'Service': [\n",
        "        'ì§ì›','ì§ì›ë¶„','ì§ì›ë“¤','ìŠ¤íƒœí”„','ì•ˆë‚´','ì‘ëŒ€','ì„œë¹„ìŠ¤','ë¶ˆì¹œì ˆ','ì¹œì ˆ','íƒœë„','ì„¤ëª…',\n",
        "        'ëŒ€ì‘','ê³ ê°ì„¼í„°','ì§ì›ì´','ë§¤ë„ˆ','ì‘ì‹œ','ì–¸í–‰','ë¶ˆì¾Œ','ë°°ë ¤','ë¬´ì‹œ','ë¶ˆë§Œ','ê´€ë¦¬ì¸'\n",
        "    ],\n",
        "    'Facility': [\n",
        "        'ì‹œì„¤','í™”ì¥ì‹¤','ì²­ê²°','ê¹¨ë—','ì •ë¹„','ìœ„ìƒ','ê´€ë¦¬','ìƒíƒœ','ëƒ„ìƒˆ','ì˜¤ì—¼','íœ´ê²Œì‹¤','ë²¤ì¹˜',\n",
        "        'ì—ì–´ì»¨','ëƒ‰ë°©','ì˜¨ë„','ê±´ë¬¼','ì˜ì','ì“°ë ˆê¸°','ë¶„ë¦¬ìˆ˜ê±°','ì£¼ì°¨ì¥','í¡ì—°','íŒŒì†','ë…¸í›„','ë‚¡ìŒ'\n",
        "    ],\n",
        "    'Price': [\n",
        "        'ê°€ê²©','ë¹„ì‹¸','ë¹„ìŒˆ','ì¿ í°','í• ì¸','ê°€ì„±ë¹„','ì…ì¥ë£Œ','ë¹„ìš©','ëˆ','í˜œíƒ','í‹°ì¼“','ìƒí’ˆê¶Œ','íŒ¨í‚¤ì§€',\n",
        "        'ìŒì‹ê°’','ë¬¼ê°€','ì‹ìŒë£Œ','ê¸°ë…í’ˆ','ë¹„ì‹¼','ì €ë ´','ë¶€ëŒ€ë¹„ìš©','ë¬´ë£Œ','ìœ ë£Œ','ëˆê°’','ê³¼ê¸ˆ'\n",
        "    ],\n",
        "    'Waiting': [\n",
        "        'ëŒ€ê¸°','ì¤„','ì›¨ì´íŒ…','ê¸°ë‹¤ë¦¼','ê¸°ë‹¤ë ¤ì•¼','ì¤„ì„œê¸°','ì¤„ì„œì„œ','ì¤„ì´ê¸¸','ë¶ë¹”','í˜¼ì¡','ì§€ì—°','ì²´ì¦',\n",
        "        'ëŒ€í˜¼ì¡','ì‹œê°„','í•œì°¸','ì˜¤ë˜','ì§€ì²´','ì •ì²´','ìˆœì„œ','ëŒ€ê¸°ì—´','ì¤„ë§ìŒ','ëŒ€ê¸°ì‹œê°„','í‘œëŒ€ê¸°','ì…ì¥ëŒ€ê¸°'\n",
        "    ],\n",
        "    'Attractions': [\n",
        "        'ë†€ì´ê¸°êµ¬','ì–´íŠ¸ë™ì…˜','í¼ë ˆì´ë“œ','ê³µì—°','ì²´í—˜','ì´ë²¤íŠ¸','íƒí—˜','ì‚¬íŒŒë¦¬','ë¶ˆê½ƒë†€ì´','ì•„ë§ˆì¡´','ìµìŠ¤í”„ë ˆìŠ¤',\n",
        "        'ìŠ¤ë¦´','ì¬ë°Œ','ì¬ë¯¸','ë¬´ì„œì›€','íƒ€ëŠ”','íƒ‘ìŠ¹','ëŒ€í˜•ê¸°êµ¬','ë™ë¬¼ì›','í¬í† ì¡´','ë†€ì´ì‹œì„¤','ë†€ì´','ë³¼ê±°ë¦¬','ì¦ê¸¸ê±°ë¦¬'\n",
        "    ]\n",
        "}\n",
        "\n",
        "theme_counts = {}\n",
        "for theme, words in theme_keywords.items():\n",
        "    count = sum(neg_text.count(w) for w in words)\n",
        "    theme_counts[theme] = count\n",
        "\n",
        "theme_df = pd.DataFrame(theme_counts.items(), columns=['Theme','Frequency']).sort_values(by='Frequency', ascending=False)\n",
        "\n",
        "print(\"\\n=== ğŸ“Š ë¶€ì • ë¦¬ë·° í…Œë§ˆë³„ ë¹ˆë„ ===\")\n",
        "print(theme_df)\n",
        "\n",
        "# ì‹œê°í™”\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(theme_df['Theme'], theme_df['Frequency'], color='salmon')\n",
        "plt.title('Negative Review Themes')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Wc2Dq1Pww-lL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "files = [\n",
        "    '/content/ì„œìš¸ëŒ€ê³µì› ì…ì¥ê° ì •ë³´_2023ë…„.xlsx',\n",
        "    '/content/ì„œìš¸ëŒ€ê³µì› ì…ì¥ê° ì •ë³´_2024ë…„ 4ì›”.xlsx',\n",
        "    '/content/ì„œìš¸ëŒ€ê³µì› ì…ì¥ê° ì •ë³´_2024ë…„ 12ì›”.xlsx',\n",
        "    '/content/ì„œìš¸ëŒ€ê³µì› ì…ì¥ê° ì •ë³´_2025ë…„ 1ì›”~3ì›”.xlsx',\n",
        "    '/content/ì„œìš¸ëŒ€ê³µì› ì…ì¥ê° ì •ë³´_2025ë…„ 1ì›”~7ì›”.xlsx'\n",
        "]\n",
        "\n",
        "df_list = []\n",
        "\n",
        "# âœ… ëª¨ë“  íŒŒì¼ ìˆœíšŒ\n",
        "for file_path in files:\n",
        "    print(f\"ğŸ“‚ ì²˜ë¦¬ ì¤‘: {file_path}\")\n",
        "    sheets = pd.ExcelFile(file_path).sheet_names\n",
        "\n",
        "    # âœ… ê° ì‹œíŠ¸ ìˆœíšŒ\n",
        "    for sheet in sheets:\n",
        "        try:\n",
        "            df = pd.read_excel(file_path, sheet_name=sheet, header=4)\n",
        "            df.columns = df.columns.map(str).str.strip()\n",
        "            df = df.dropna(axis=1, how='all')\n",
        "            df = df.dropna(subset=['ë‚ ì§œ'], how='all')\n",
        "            df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'], errors='coerce')\n",
        "\n",
        "            # ğŸ”¹ í…Œë§ˆê°€ë“  ê´€ë ¨ ì»¬ëŸ¼ë§Œ ì„ íƒ (.1ì´ ë¶™ì€ ì—´ë“¤)\n",
        "            theme_cols = ['ë‚ ì§œ','ìš”ì¼','ìœ ë£Œê³„.1','ì–´ë¥¸.1','ì²­ì†Œë…„.1','ì–´ë¦°ì´.1','ì™¸êµ­ì¸.1','ë‹¨ì²´ì…ì¥.1']\n",
        "            theme_exist = [c for c in theme_cols if c in df.columns]\n",
        "\n",
        "            if len(theme_exist) < 2:\n",
        "                continue  # í•´ë‹¹ ì‹œíŠ¸ì— í…Œë§ˆê°€ë“  ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€\n",
        "\n",
        "            temp = df[theme_exist].copy()\n",
        "\n",
        "            # ğŸ”¹ ì»¬ëŸ¼ ì´ë¦„ ì •ë¦¬\n",
        "            temp = temp.rename(columns={\n",
        "                'ìœ ë£Œê³„.1': 'ìœ ë£Œê³„',\n",
        "                'ì–´ë¥¸.1': 'ì–´ë¥¸',\n",
        "                'ì²­ì†Œë…„.1': 'ì²­ì†Œë…„',\n",
        "                'ì–´ë¦°ì´.1': 'ì–´ë¦°ì´',\n",
        "                'ì™¸êµ­ì¸.1': 'ì™¸êµ­ì¸',\n",
        "                'ë‹¨ì²´ì…ì¥.1': 'ë‹¨ì²´ì…ì¥'\n",
        "            })\n",
        "\n",
        "            temp['êµ¬ì—­'] = 'í…Œë§ˆê°€ë“ '\n",
        "            df_list.append(temp)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ {file_path} ({sheet}) ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜:\", e)\n",
        "\n",
        "# âœ… ì „ì²´ ë³‘í•© ë° ì •ë ¬\n",
        "theme_df = pd.concat(df_list, ignore_index=True)\n",
        "theme_df = theme_df.sort_values('ë‚ ì§œ')\n",
        "\n",
        "# âœ… ê²°ê³¼ í™•ì¸\n",
        "print(f\"\\nâœ… ì´ {len(theme_df)}í–‰ ë³‘í•© ì™„ë£Œ!\")\n",
        "print(theme_df.head())\n",
        "\n",
        "# âœ… CSV ì €ì¥\n",
        "output_path = '/content/ì„œìš¸ëŒ€ê³µì›_í…Œë§ˆê°€ë“ _ì…ì¥ê°_2023_2025.csv'\n",
        "theme_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_path}\")\n"
      ],
      "metadata": {
        "id": "KE4wiK522H4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "GhN_BsYNlxMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/waiting_times.csv')"
      ],
      "metadata": {
        "id": "CwNmFdofl1DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lY_tL8pGWKul"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}